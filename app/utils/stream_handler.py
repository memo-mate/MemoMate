import os
from typing import Any

from langchain_community.callbacks.openai_info import OpenAICallbackHandler
from langchain_core.documents import Document
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.messages.ai import UsageMetadata
from langchain_openai import ChatOpenAI
from rich import box
from rich.console import Console, Group
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Prompt
from rich.text import Text

from app.configs import settings

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆ›å»ºä¸€ä¸ªå…¨å±€çš„æ§åˆ¶å°å¯¹è±¡
console = Console(color_system="auto")


class RichStreamingCallbackHandler(OpenAICallbackHandler):
    """ä½¿ç”¨Richå®ç°å®Œæ•´æµç¨‹å¯è§†åŒ–çš„å›è°ƒå¤„ç†å™¨"""

    def __init__(self, robot_name: str):
        super().__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•ä»¥ç¡®ä¿tokenè®¡æ•°åŠŸèƒ½æ­£å¸¸
        self.console = Console()
        self.robot_name = robot_name
        self.text = ""
        self.llm_live = None
        self.retrieval_results = []
        self.retrieval_live = None

    def on_retriever_start(self, serialized: dict[str, Any], query: str, *args, **kwargs):
        """æ£€ç´¢å¼€å§‹æ—¶çš„å›è°ƒ"""
        self.__retriever_title = f"ğŸ” çŸ¥è¯†æ£€ç´¢ç»“æœ - {query}"
        self.retrieval_results = []
        self.text = ""
        self.retrieval_live = Live(
            Panel(
                Group(Text("æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...", style="yellow")),
                title=self.__retriever_title,
                border_style="blue",
                box=box.ROUNDED,
                width=200,
                expand=True,
            ),
            refresh_per_second=4,
            console=self.console,
            vertical_overflow="visible",
        )
        self.retrieval_live.start()

    def on_retriever_end(self, documents: list[Document], *args, **kwargs):
        """æ£€ç´¢ç»“æŸæ—¶çš„å›è°ƒ"""
        if self.retrieval_live:
            # æ„å»ºæœ€ç»ˆæ˜¾ç¤ºå†…å®¹
            group_components = []
            if documents:
                for i, doc in enumerate(documents, 1):
                    group_components.append(Text(f"\næ–‡æ¡£ç‰‡æ®µ {i}: {doc.page_content}", style="bold blue"))
            else:
                group_components.append(Text("æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯", style="yellow"))

            # ä½¿ç”¨Groupç»„åˆæ‰€æœ‰ç»„ä»¶
            self.retrieval_live.update(
                Panel(
                    Group(*group_components),
                    title=self.__retriever_title,
                    border_style="blue",
                    box=box.ROUNDED,
                    width=200,
                    expand=True,
                )
            )
            self.retrieval_live.stop()

    def on_llm_start(self, *args, **kwargs):
        """LLMå¼€å§‹ç”Ÿæˆæ—¶çš„å›è°ƒ"""
        self.text = ""
        self.llm_live = Live(
            Panel(
                Text("æ­£åœ¨æ€è€ƒå›ç­”...", style="yellow"),
                title=f"ğŸ¤– {self.robot_name}",
                border_style="cyan",
                box=box.ROUNDED,
            ),
            refresh_per_second=4,
            console=self.console,
            vertical_overflow="visible",
        )
        self.llm_live.start()

    def on_llm_new_token(self, token: str, **kwargs):
        """æ¥æ”¶åˆ°æ–°tokenæ—¶çš„å›è°ƒ"""
        self.text += token
        if not token:
            return

        self.llm_live.update(
            Panel(
                Markdown(self.text),
                title=f"ğŸ¤– {self.robot_name}",
                border_style="cyan",
                box=box.ROUNDED,
                width=200,
                expand=True,
            ),
        )
        # æ‰‹åŠ¨è§¦å‘æ»šåŠ¨ - ä½¿ç”¨æ§åˆ¶å°çš„printæ–¹æ³•ä½†ä¸å®é™…è¾“å‡ºå†…å®¹
        self.console.print("", end="")

    def on_llm_end(self, *args, **kwargs):
        """LLMç”Ÿæˆç»“æŸæ—¶çš„å›è°ƒ"""
        super().on_llm_end(*args, **kwargs)
        # åˆ›å»ºtokenä¿¡æ¯æ ‡é¢˜
        token_info = f"è¾“å…¥: {self.prompt_tokens} tokens | è¾“å‡º: {self.completion_tokens} tokens"

        # æ›´æ–°é¢æ¿æ˜¾ç¤ºtokenä¿¡æ¯
        if self.llm_live:
            self.llm_live.update(
                Panel(
                    Markdown(self.text),
                    title=f"ğŸ¤– {self.robot_name}",
                    subtitle=token_info,
                    subtitle_align="center",
                    border_style="cyan",
                    box=box.ROUNDED,
                    width=200,
                    expand=True,
                )
            )
            self.llm_live.stop()
            console.print(f"{self}\n")


class Assistant:
    def __init__(self, project_name: str, robot_name: str, user: str):
        self.project_name = project_name
        self.robot_name = robot_name
        self.user = user

    def pdf_agent_stream(self, user: str = "user"):
        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        streaming_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        # åˆ›å»ºLLM
        llm = ChatOpenAI(
            model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            api_key=settings.openai_api_key,
            base_url="https://api.siliconflow.cn/v1",
            temperature=0,
            max_tokens=3200,
            timeout=None,
            max_retries=3,
            streaming=True,  # æµå¼è¾“å‡º
            callbacks=[streaming_handler],
        )

        # äº¤äº’å¾ªç¯
        console.print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # è°ƒç”¨é“¾è¿›è¡Œæµå¼è¾“å‡º - ä¿®æ”¹è¿™é‡Œ
            template = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä»¥markdownæ ¼å¼è¾“å‡ºã€‚"),
                HumanMessage(content=message),
            ]
            response: AIMessage = llm.invoke(template, config={"callbacks": [streaming_handler]})
            useage: UsageMetadata = response.usage_metadata
            streaming_handler.completion_tokens = useage["output_tokens"]
            streaming_handler.prompt_tokens = useage["input_tokens"]
            streaming_handler.total_tokens = useage["total_tokens"]


if __name__ == "__main__":
    assistant = Assistant(project_name="é­…é­”éª‚ä»–", robot_name="å°é­…", user="daoji")
    assistant.pdf_agent_stream()
