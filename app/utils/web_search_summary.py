import asyncio
import operator
from abc import ABC, abstractmethod
from enum import Enum
from pathlib import Path
from typing import Annotated, Any, Literal, TypedDict

from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
from langchain.chat_models import init_chat_model
from langchain_community.document_loaders import (
    CSVLoader,
    Docx2txtLoader,
    JSONLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    WebBaseLoader,
)
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import TokenTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from pydantic import BaseModel, Field

from app.core import settings
from app.core.log_adapter import logger

# ==================== å¼‚å¸¸å®šä¹‰ ====================


class HumanReviewRequiredException(Exception):
    """éœ€è¦äººå·¥å®¡æ ¸çš„å¼‚å¸¸"""

    def __init__(self, message: str, summary_preview: str = "", state: dict = None):
        super().__init__(message)
        self.summary_preview = summary_preview
        self.state = state


# ==================== é…ç½®å’Œæšä¸¾ ====================


class SummaryStrategy(str, Enum):
    """æ€»ç»“ç­–ç•¥æšä¸¾"""

    CONCISE = "concise"  # ç®€æ´æ‘˜è¦
    DETAILED = "detailed"  # è¯¦ç»†æ‘˜è¦
    BULLET_POINTS = "bullet_points"  # è¦ç‚¹æ‘˜è¦
    TECHNICAL = "technical"  # æŠ€æœ¯æ‘˜è¦


class DocumentFormat(str, Enum):
    """æ”¯æŒçš„æ–‡æ¡£æ ¼å¼"""

    WEB = "web"
    PDF = "pdf"
    DOCX = "docx"
    TXT = "txt"
    MD = "markdown"
    CSV = "csv"
    JSON = "json"


class ProcessingConfig(BaseModel):
    """å¤„ç†é…ç½®"""

    token_max: int = Field(default=3000, description="æœ€å¤§tokené™åˆ¶")
    chunk_size: int = Field(default=1000, description="æ–‡æ¡£åˆ†å—å¤§å°")
    chunk_overlap: int = Field(default=200, description="åˆ†å—é‡å å¤§å°")
    temperature: float = Field(default=0, description="LLMæ¸©åº¦å‚æ•°")
    recursion_limit: int = Field(default=10, description="é€’å½’é™åˆ¶")
    enable_cache: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨ç¼“å­˜")
    enable_human_review: bool = Field(default=False, description="æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸")
    interactive_review: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨äº¤äº’å¼å®¡æ ¸ï¼ˆå‘½ä»¤è¡Œè¾“å…¥ï¼‰")


# ==================== LLM åˆå§‹åŒ– ====================


def create_llm(config: ProcessingConfig):
    """åˆ›å»ºLLMå®ä¾‹"""
    try:
        return init_chat_model(
            "deepseek-ai/DeepSeek-R1",
            model_provider="openai",
            temperature=config.temperature,
            api_key=settings.SILICONFLOW_API_KEY,
            base_url=settings.SILICONFLOW_API_BASE,
        )
    except Exception as e:
        logger.exception("åˆå§‹åŒ–LLMå¤±è´¥", exc_info=e)
        raise


# ==================== æç¤ºæ¨¡æ¿ç­–ç•¥ ====================


class PromptTemplateStrategy(ABC):
    """æç¤ºæ¨¡æ¿ç­–ç•¥åŸºç±»"""

    @abstractmethod
    def get_map_template(self) -> str:
        """è·å–Mapé˜¶æ®µçš„æç¤ºæ¨¡æ¿"""
        pass

    @abstractmethod
    def get_reduce_template(self) -> str:
        """è·å–Reduceé˜¶æ®µçš„æç¤ºæ¨¡æ¿"""
        pass


class ConciseStrategy(PromptTemplateStrategy):
    """ç®€æ´æ‘˜è¦ç­–ç•¥"""

    def get_map_template(self) -> str:
        return "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹ï¼š\n{context}"

    def get_reduce_template(self) -> str:
        return """
ä»¥ä¸‹æ˜¯ä¸€ç»„æ‘˜è¦ï¼š
{docs}

è¯·å°†è¿™äº›æ‘˜è¦æ•´åˆæˆä¸€ä¸ªæœ€ç»ˆçš„ç®€æ´æ‘˜è¦ï¼Œçªå‡ºä¸»è¦ä¸»é¢˜å’Œå…³é”®ç‚¹ã€‚
å›ç­”è¯·ä½¿ç”¨ä¸­æ–‡ã€‚
"""


class DetailedStrategy(PromptTemplateStrategy):
    """è¯¦ç»†æ‘˜è¦ç­–ç•¥"""

    def get_map_template(self) -> str:
        return "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆè¯¦ç»†çš„æ‘˜è¦ï¼ŒåŒ…å«é‡è¦ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯ï¼š\n{context}"

    def get_reduce_template(self) -> str:
        return """
ä»¥ä¸‹æ˜¯ä¸€ç»„è¯¦ç»†æ‘˜è¦ï¼š
{docs}

è¯·å°†è¿™äº›æ‘˜è¦æ•´åˆæˆä¸€ä¸ªå…¨é¢çš„æœ€ç»ˆæ‘˜è¦ï¼Œä¿ç•™é‡è¦ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯ã€‚
å›ç­”è¯·ä½¿ç”¨ä¸­æ–‡ã€‚
"""


class BulletPointsStrategy(PromptTemplateStrategy):
    """è¦ç‚¹æ‘˜è¦ç­–ç•¥"""

    def get_map_template(self) -> str:
        return "è¯·å°†ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹æ•´ç†æˆè¦ç‚¹åˆ—è¡¨ï¼š\n{context}"

    def get_reduce_template(self) -> str:
        return """
ä»¥ä¸‹æ˜¯ä¸€ç»„è¦ç‚¹æ‘˜è¦ï¼š
{docs}

è¯·å°†è¿™äº›è¦ç‚¹æ•´åˆå¹¶å»é‡ï¼Œå½¢æˆä¸€ä¸ªç»“æ„åŒ–çš„æœ€ç»ˆè¦ç‚¹åˆ—è¡¨ã€‚
å›ç­”è¯·ä½¿ç”¨ä¸­æ–‡ï¼Œä½¿ç”¨é¡¹ç›®ç¬¦å·æ ¼å¼ã€‚
"""


class TechnicalStrategy(PromptTemplateStrategy):
    """æŠ€æœ¯æ‘˜è¦ç­–ç•¥"""

    def get_map_template(self) -> str:
        return "è¯·ä¸ºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ï¼Œé‡ç‚¹å…³æ³¨æŠ€æœ¯ç»†èŠ‚ã€æ–¹æ³•å’Œå®ç°ï¼š\n{context}"

    def get_reduce_template(self) -> str:
        return """
ä»¥ä¸‹æ˜¯ä¸€ç»„æŠ€æœ¯æ‘˜è¦ï¼š
{docs}

è¯·å°†è¿™äº›æŠ€æœ¯æ‘˜è¦æ•´åˆæˆä¸€ä¸ªå…¨é¢çš„æŠ€æœ¯æ€»ç»“ï¼Œä¿ç•™å…³é”®çš„æŠ€æœ¯ç»†èŠ‚å’Œæ–¹æ³•ã€‚
å›ç­”è¯·ä½¿ç”¨ä¸­æ–‡ã€‚
"""


# ==================== æ–‡æ¡£åŠ è½½å™¨å·¥å‚ ====================


class DocumentLoaderFactory:
    """æ–‡æ¡£åŠ è½½å™¨å·¥å‚"""

    @staticmethod
    def create_loader(source: str | Path, doc_format: DocumentFormat):
        """æ ¹æ®æ ¼å¼åˆ›å»ºå¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨"""
        loaders = {
            DocumentFormat.WEB: lambda: WebBaseLoader(str(source)),
            DocumentFormat.PDF: lambda: PyPDFLoader(str(source)),
            DocumentFormat.DOCX: lambda: Docx2txtLoader(str(source)),
            DocumentFormat.TXT: lambda: TextLoader(str(source)),
            DocumentFormat.MD: lambda: UnstructuredMarkdownLoader(str(source)),
            DocumentFormat.CSV: lambda: CSVLoader(str(source)),
            DocumentFormat.JSON: lambda: JSONLoader(str(source)),
        }

        if doc_format not in loaders:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {doc_format}")

        return loaders[doc_format]()


# ==================== çŠ¶æ€å®šä¹‰ ====================


class OverallState(TypedDict):
    """ä¸»å›¾çš„æ•´ä½“çŠ¶æ€"""

    contents: list[str]  # è¾“å…¥çš„æ–‡æ¡£å†…å®¹
    summaries: Annotated[list[str], operator.add]  # ç”Ÿæˆçš„æ‘˜è¦åˆ—è¡¨
    collapsed_summaries: list[Document]  # æŠ˜å åçš„æ‘˜è¦
    final_summary: str  # æœ€ç»ˆæ‘˜è¦
    config: ProcessingConfig  # å¤„ç†é…ç½®
    strategy: str  # æ‘˜è¦ç­–ç•¥
    metadata: dict[str, Any]  # å…ƒæ•°æ®
    human_review_required: bool  # æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    processing_steps: list[str]  # å¤„ç†æ­¥éª¤è®°å½•


class SummaryState(TypedDict):
    """å•ä¸ªæ‘˜è¦èŠ‚ç‚¹çš„çŠ¶æ€"""

    content: str
    config: ProcessingConfig
    strategy: str


class HumanReviewState(TypedDict):
    """äººå·¥å®¡æ ¸çŠ¶æ€"""

    summary: str
    approved: bool
    feedback: str


# ==================== æ ¸å¿ƒå¤„ç†ç±» ====================


class WebSearchProcessor:
    """Webæœç´¢å’Œæ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, config: ProcessingConfig = None):
        self.config = config or ProcessingConfig()
        self.llm = create_llm(self.config)
        self.strategy_map = {
            SummaryStrategy.CONCISE: ConciseStrategy(),
            SummaryStrategy.DETAILED: DetailedStrategy(),
            SummaryStrategy.BULLET_POINTS: BulletPointsStrategy(),
            SummaryStrategy.TECHNICAL: TechnicalStrategy(),
        }
        self.memory = MemorySaver() if self.config.enable_cache else None
        self.human_review_handler = None  # äººå·¥å®¡æ ¸å¤„ç†å™¨

    def get_strategy(self, strategy_name: str) -> PromptTemplateStrategy:
        """è·å–æ‘˜è¦ç­–ç•¥"""
        strategy = SummaryStrategy(strategy_name)
        return self.strategy_map[strategy]

    def set_human_review_handler(self, handler):
        """è®¾ç½®äººå·¥å®¡æ ¸å¤„ç†å™¨

        Args:
            handler: å¼‚æ­¥å‡½æ•°ï¼Œæ¥æ”¶stateå‚æ•°ï¼Œè¿”å›åŒ…å«approvedå’Œfeedbackå­—æ®µçš„å­—å…¸
        """
        self.human_review_handler = handler

    async def _interactive_review(self, state: OverallState) -> dict:
        """å†…ç½®çš„äº¤äº’å¼å®¡æ ¸åŠŸèƒ½"""
        summaries = state["collapsed_summaries"]
        metadata = state.get("metadata", {})

        print("\n" + "=" * 60)
        print("ğŸ“‹ éœ€è¦äººå·¥å®¡æ ¸çš„å†…å®¹")
        print("=" * 60)

        print("ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   - æ–‡æ¡£æ•°é‡: {len(summaries)}")
        print(f"   - å…ƒæ•°æ®: {metadata}")

        print("\nğŸ“„ å†…å®¹è¯¦æƒ…:")
        for i, doc in enumerate(summaries, 1):
            content = doc.page_content
            preview = content if len(content) <= 100 else content[:100] + "..."
            print(f"   {i}. {preview}")
            print(f"      (å®Œæ•´é•¿åº¦: {len(content)} å­—ç¬¦)")

        # å†…å®¹åˆ†æ
        all_content = " ".join([doc.page_content for doc in summaries])
        total_length = len(all_content)

        print("\nğŸ” å†…å®¹åˆ†æ:")
        print(f"   - æ€»å­—ç¬¦æ•°: {total_length}")

        # æ•æ„Ÿè¯æ£€æµ‹
        sensitive_words = ["è´¢åŠ¡", "æ•æ„Ÿ", "æœºå¯†", "å†…éƒ¨", "å¯†ç ", "è´¦å·", "ç§æœ‰"]
        found_sensitive = [word for word in sensitive_words if word in all_content]
        if found_sensitive:
            print(f"   - âš ï¸  å‘ç°æ•æ„Ÿè¯: {found_sensitive}")
        else:
            print("   - âœ… æœªå‘ç°æ•æ„Ÿè¯")

        # è´¨é‡æŒ‡æ ‡
        quality_words = ["æŠ¥å‘Š", "åˆ†æ", "è¯„ä¼°", "è§„åˆ’", "æ€»ç»“", "å»ºè®®", "æ–¹æ¡ˆ", "ç­–ç•¥"]
        quality_count = sum(1 for word in quality_words if word in all_content)
        print(f"   - ğŸ“ˆ è´¨é‡æŒ‡æ ‡: {quality_count}/{len(quality_words)}")

        print("=" * 60)

        # è·å–ç”¨æˆ·å†³ç­–
        while True:
            print("\nğŸ¤” è¯·åšå‡ºå®¡æ ¸å†³ç­–:")
            print("   1. é€šè¿‡ (è¾“å…¥ 'y', 'yes', '1', 'é€šè¿‡')")
            print("   2. æ‹’ç» (è¾“å…¥ 'n', 'no', '2', 'æ‹’ç»')")
            print("   3. æŸ¥çœ‹å®Œæ•´å†…å®¹ (è¾“å…¥ 'view', 'v', 'æŸ¥çœ‹')")
            print("   4. é€€å‡ºç¨‹åº (è¾“å…¥ 'quit', 'q', 'é€€å‡º')")

            try:
                choice = input("\nğŸ‘‰ è¯·è¾“å…¥ä½ çš„é€‰æ‹©: ").strip().lower()

                if choice in ["y", "yes", "1", "é€šè¿‡"]:
                    feedback = input("ğŸ’¬ è¯·è¾“å…¥é€šè¿‡ç†ç”± (å¯é€‰): ").strip()
                    return {
                        "approved": True,
                        "feedback": feedback or "äººå·¥å®¡æ ¸é€šè¿‡",
                        "reviewer": "interactive_human",
                        "review_details": {
                            "sensitive_words": found_sensitive,
                            "quality_score": quality_count,
                            "total_length": total_length,
                        },
                    }

                elif choice in ["n", "no", "2", "æ‹’ç»"]:
                    feedback = input("ğŸ’¬ è¯·è¾“å…¥æ‹’ç»ç†ç”± (å¿…å¡«): ").strip()
                    if not feedback:
                        print("âŒ æ‹’ç»æ—¶å¿…é¡»æä¾›ç†ç”±ï¼Œè¯·é‡æ–°è¾“å…¥")
                        continue
                    return {
                        "approved": False,
                        "feedback": feedback,
                        "reviewer": "interactive_human",
                        "review_details": {
                            "sensitive_words": found_sensitive,
                            "quality_score": quality_count,
                            "total_length": total_length,
                        },
                    }

                elif choice in ["view", "v", "æŸ¥çœ‹"]:
                    print("\n" + "=" * 60)
                    print("ğŸ“‹ å®Œæ•´å†…å®¹è¯¦æƒ…")
                    print("=" * 60)

                    for i, doc in enumerate(summaries, 1):
                        print(f"\nğŸ“„ æ–‡æ¡£ {i}:")
                        print(f"   {doc.page_content}")
                        if hasattr(doc, "metadata") and doc.metadata:
                            print(f"   å…ƒæ•°æ®: {doc.metadata}")

                    print("\næŒ‰å›è½¦é”®ç»§ç»­...")
                    input()
                    continue

                elif choice in ["quit", "q", "é€€å‡º"]:
                    print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                    import sys

                    sys.exit(0)

                else:
                    print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°é€‰æ‹©")
                    continue

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­")
                import sys

                sys.exit(0)
            except EOFError:
                print("\nğŸ‘‹ è¾“å…¥ç»“æŸï¼Œé€€å‡ºç¨‹åº")
                import sys

                sys.exit(0)

    def length_function(self, documents: list[Document]) -> int:
        """è®¡ç®—æ–‡æ¡£åˆ—è¡¨çš„tokenæ•°é‡"""
        try:
            return sum(self.llm.get_num_tokens(doc.page_content) for doc in documents)
        except Exception as e:
            logger.exception("è®¡ç®—tokenæ•°é‡å¤±è´¥", exc_info=e)
            # fallback: ä¼°ç®—tokenæ•°é‡
            return sum(len(doc.page_content.split()) * 1.3 for doc in documents)

    async def generate_summary(self, state: SummaryState) -> dict:
        """ç”Ÿæˆå•ä¸ªæ–‡æ¡£çš„æ‘˜è¦"""
        try:
            strategy = self.get_strategy(state["strategy"])
            map_template = strategy.get_map_template()
            map_prompt = ChatPromptTemplate([("human", map_template)])
            map_chain = map_prompt | self.llm | StrOutputParser()

            response = await map_chain.ainvoke({"context": state["content"]})
            logger.info("ç”Ÿæˆå•ä¸ªæ‘˜è¦æˆåŠŸ", content_length=len(state["content"]))
            return {"summaries": [response]}
        except Exception as e:
            logger.exception("ç”Ÿæˆæ‘˜è¦å¤±è´¥", exc_info=e)
            return {"summaries": [f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"]}

    def map_summaries(self, state: OverallState) -> list[Send]:
        """æ˜ å°„å‡½æ•°ï¼šä¸ºæ¯ä¸ªæ–‡æ¡£å†…å®¹åˆ›å»ºSendå¯¹è±¡"""
        return [
            Send("generate_summary", {"content": content, "config": state["config"], "strategy": state["strategy"]})
            for content in state["contents"]
        ]

    def collect_summaries(self, state: OverallState) -> dict:
        """æ”¶é›†æ‰€æœ‰æ‘˜è¦å¹¶è½¬æ¢ä¸ºDocumentå¯¹è±¡"""
        collapsed_summaries = [Document(page_content=summary) for summary in state["summaries"]]
        processing_steps = state.get("processing_steps", [])
        processing_steps.append(f"æ”¶é›†äº† {len(state['summaries'])} ä¸ªæ‘˜è¦")

        return {"collapsed_summaries": collapsed_summaries, "processing_steps": processing_steps}

    async def collapse_summaries(self, state: OverallState) -> dict:
        """æŠ˜å æ‘˜è¦ï¼šå°†è¿‡é•¿çš„æ‘˜è¦åˆ—è¡¨è¿›ä¸€æ­¥æ€»ç»“"""
        try:
            strategy = self.get_strategy(state["strategy"])
            reduce_template = strategy.get_reduce_template()
            reduce_prompt = ChatPromptTemplate([("human", reduce_template)])
            reduce_chain = reduce_prompt | self.llm | StrOutputParser()

            # æ ¹æ®tokené™åˆ¶åˆ†å‰²æ–‡æ¡£åˆ—è¡¨
            doc_lists = split_list_of_docs(
                state["collapsed_summaries"], self.length_function, state["config"].token_max
            )

            results = []
            for doc_list in doc_lists:
                collapsed_doc = await acollapse_docs(doc_list, reduce_chain.ainvoke)
                results.append(collapsed_doc)

            processing_steps = state.get("processing_steps", [])
            processing_steps.append(f"æŠ˜å æ‘˜è¦: {len(state['collapsed_summaries'])} -> {len(results)}")

            logger.info("æŠ˜å æ‘˜è¦æˆåŠŸ", original_count=len(state["collapsed_summaries"]), collapsed_count=len(results))

            return {"collapsed_summaries": results, "processing_steps": processing_steps}
        except Exception as e:
            logger.exception("æŠ˜å æ‘˜è¦å¤±è´¥", exc_info=e)
            return {"collapsed_summaries": state["collapsed_summaries"]}

    def should_collapse(
        self, state: OverallState
    ) -> Literal["collapse_summaries", "human_review", "generate_final_summary"]:
        """å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
        num_tokens = self.length_function(state["collapsed_summaries"])

        if num_tokens > state["config"].token_max:
            return "collapse_summaries"
        elif state["config"].enable_human_review and not state.get("human_review_required", False):
            return "human_review"
        else:
            return "generate_final_summary"

    async def human_review(self, state: OverallState) -> dict:
        """äººå·¥å®¡æ ¸èŠ‚ç‚¹"""
        summary_preview = "\n".join([doc.page_content[:200] + "..." for doc in state["collapsed_summaries"]])

        logger.info("éœ€è¦äººå·¥å®¡æ ¸", summary_preview=summary_preview[:500])

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº¤äº’å¼å®¡æ ¸
        if self.config.interactive_review:
            try:
                # ä½¿ç”¨å†…ç½®çš„äº¤äº’å¼å®¡æ ¸
                review_result = await self._interactive_review(state)

                return {
                    "human_review_required": True,
                    "metadata": {
                        **state.get("metadata", {}),
                        "human_review_status": "completed",
                        "review_timestamp": asyncio.get_event_loop().time(),
                        "review_approved": review_result.get("approved", True),
                        "review_feedback": review_result.get("feedback", ""),
                        "reviewer": review_result.get("reviewer", "interactive_human"),
                        "review_details": review_result.get("review_details", {}),
                    },
                }
            except Exception as e:
                logger.exception("äº¤äº’å¼å®¡æ ¸æ‰§è¡Œå¤±è´¥", exc_info=e)
                # å¦‚æœäº¤äº’å¼å®¡æ ¸å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise HumanReviewRequiredException(
                    "éœ€è¦äººå·¥å®¡æ ¸ï¼Œä½†äº¤äº’å¼å®¡æ ¸æ‰§è¡Œå¤±è´¥", summary_preview=summary_preview, state=state
                )

        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„å®¡æ ¸å¤„ç†å™¨
        if hasattr(self, "human_review_handler") and self.human_review_handler:
            try:
                # è°ƒç”¨è‡ªå®šä¹‰çš„äººå·¥å®¡æ ¸å¤„ç†å™¨
                review_result = await self.human_review_handler(state)

                return {
                    "human_review_required": True,
                    "metadata": {
                        **state.get("metadata", {}),
                        "human_review_status": "completed",
                        "review_timestamp": asyncio.get_event_loop().time(),
                        "review_approved": review_result.get("approved", True),
                        "review_feedback": review_result.get("feedback", ""),
                        "reviewer": review_result.get("reviewer", "custom_handler"),
                        "review_details": review_result.get("review_details", {}),
                    },
                }
            except Exception as e:
                logger.exception("äººå·¥å®¡æ ¸å¤„ç†å™¨æ‰§è¡Œå¤±è´¥", exc_info=e)
                # å¦‚æœå®¡æ ¸å¤„ç†å™¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è¦æ±‚æ‰‹åŠ¨å¤„ç†
                raise HumanReviewRequiredException(
                    "éœ€è¦äººå·¥å®¡æ ¸ï¼Œä½†å®¡æ ¸å¤„ç†å™¨æ‰§è¡Œå¤±è´¥", summary_preview=summary_preview, state=state
                )

        # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰å¤„ç†å™¨ä¸”æœªå¯ç”¨äº¤äº’å¼å®¡æ ¸ï¼ŒæŠ›å‡ºå¼‚å¸¸è¦æ±‚äººå·¥ä»‹å…¥
        raise HumanReviewRequiredException("éœ€è¦äººå·¥å®¡æ ¸ï¼Œè¯·å¤„ç†åç»§ç»­", summary_preview=summary_preview, state=state)

    async def generate_final_summary(self, state: OverallState) -> dict:
        """ç”Ÿæˆæœ€ç»ˆæ‘˜è¦"""
        try:
            strategy = self.get_strategy(state["strategy"])
            reduce_template = strategy.get_reduce_template()
            reduce_prompt = ChatPromptTemplate([("human", reduce_template)])
            reduce_chain = reduce_prompt | self.llm | StrOutputParser()

            summary_texts = [doc.page_content for doc in state["collapsed_summaries"]]
            response = await reduce_chain.ainvoke({"docs": "\n".join(summary_texts)})

            processing_steps = state.get("processing_steps", [])
            processing_steps.append("ç”Ÿæˆæœ€ç»ˆæ‘˜è¦å®Œæˆ")

            logger.info("ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æˆåŠŸ", summary_length=len(response))

            return {
                "final_summary": response,
                "processing_steps": processing_steps,
                "metadata": {
                    **state.get("metadata", {}),
                    "final_summary_length": len(response),
                    "total_processing_steps": len(processing_steps),
                },
            }
        except Exception as e:
            logger.exception("ç”Ÿæˆæœ€ç»ˆæ‘˜è¦å¤±è´¥", exc_info=e)
            return {"final_summary": f"æœ€ç»ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"}

    def create_map_reduce_graph(self):
        """åˆ›å»ºä¼˜åŒ–çš„Map-Reduceå›¾"""
        graph = StateGraph(OverallState)

        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("generate_summary", self.generate_summary)
        graph.add_node("collect_summaries", self.collect_summaries)
        graph.add_node("collapse_summaries", self.collapse_summaries)
        graph.add_node("human_review", self.human_review)
        graph.add_node("generate_final_summary", self.generate_final_summary)

        # æ·»åŠ è¾¹
        graph.add_conditional_edges(START, self.map_summaries, ["generate_summary"])
        graph.add_edge("generate_summary", "collect_summaries")
        graph.add_conditional_edges("collect_summaries", self.should_collapse)
        graph.add_conditional_edges("collapse_summaries", self.should_collapse)
        graph.add_edge("human_review", "generate_final_summary")
        graph.add_edge("generate_final_summary", END)

        # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œæ·»åŠ æ£€æŸ¥ç‚¹
        compile_config = {}
        if self.memory:
            compile_config["checkpointer"] = self.memory

        return graph.compile(**compile_config)

    async def process_documents(
        self,
        source: str | Path | list[str],
        doc_format: DocumentFormat = DocumentFormat.WEB,
        strategy: SummaryStrategy = SummaryStrategy.CONCISE,
        custom_config: ProcessingConfig | None = None,
    ) -> dict[str, Any]:
        """å¤„ç†æ–‡æ¡£çš„ä¸»è¦æ¥å£"""
        config = custom_config or self.config

        try:
            # åŠ è½½æ–‡æ¡£
            if isinstance(source, list):
                contents = source
            else:
                loader = DocumentLoaderFactory.create_loader(source, doc_format)
                documents = loader.load()

                # åˆ†å‰²æ–‡æ¡£
                text_splitter = TokenTextSplitter.from_tiktoken_encoder(
                    chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap
                )
                split_docs = text_splitter.split_documents(documents)
                contents = [doc.page_content for doc in split_docs]

            logger.info(
                "å¼€å§‹å¤„ç†æ–‡æ¡£", source=str(source), doc_format=doc_format, strategy=strategy, chunk_count=len(contents)
            )

            # åˆ›å»ºå¤„ç†å›¾
            app = self.create_map_reduce_graph()

            # åˆå§‹çŠ¶æ€
            initial_state = {
                "contents": contents,
                "config": config,
                "strategy": strategy.value,
                "metadata": {
                    "source": str(source),
                    "doc_format": doc_format.value,
                    "strategy": strategy.value,
                    "chunk_count": len(contents),
                    "start_time": asyncio.get_event_loop().time(),
                },
                "processing_steps": [f"å¼€å§‹å¤„ç† {len(contents)} ä¸ªæ–‡æ¡£å—"],
            }

            # æ‰§è¡Œå¤„ç†
            final_state = None
            async for step in app.astream(initial_state, {"recursion_limit": config.recursion_limit}):
                step_name = list(step.keys())[0]
                logger.info("æ‰§è¡Œå¤„ç†æ­¥éª¤", step_name=step_name)
                final_state = step[step_name]

            # æ·»åŠ å®Œæˆæ—¶é—´
            if final_state and "metadata" in final_state:
                final_state["metadata"]["end_time"] = asyncio.get_event_loop().time()
                final_state["metadata"]["processing_duration"] = (
                    final_state["metadata"]["end_time"] - final_state["metadata"]["start_time"]
                )

            logger.info(
                "æ–‡æ¡£å¤„ç†å®Œæˆ",
                processing_duration=final_state["metadata"].get("processing_duration"),
                final_summary_length=len(final_state.get("final_summary", "")),
            )

            return final_state

        except Exception as e:
            logger.exception("æ–‡æ¡£å¤„ç†å¤±è´¥", source=str(source), exc_info=e)
            raise


# ==================== ä¾¿æ·å‡½æ•° ====================


async def summarize_web_content(
    url: str, strategy: SummaryStrategy = SummaryStrategy.CONCISE, config: ProcessingConfig | None = None
) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šæ€»ç»“ç½‘é¡µå†…å®¹"""
    processor = WebSearchProcessor(config)
    result = await processor.process_documents(url, DocumentFormat.WEB, strategy)
    return result.get("final_summary", "")


async def summarize_document(
    file_path: str | Path,
    doc_format: DocumentFormat,
    strategy: SummaryStrategy = SummaryStrategy.CONCISE,
    config: ProcessingConfig | None = None,
) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šæ€»ç»“æ–‡æ¡£"""
    processor = WebSearchProcessor(config)
    result = await processor.process_documents(file_path, doc_format, strategy)
    return result.get("final_summary", "")


async def summarize_text_list(
    texts: list[str], strategy: SummaryStrategy = SummaryStrategy.CONCISE, config: ProcessingConfig | None = None
) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šæ€»ç»“æ–‡æœ¬åˆ—è¡¨"""
    processor = WebSearchProcessor(config)
    result = await processor.process_documents(texts, strategy=strategy)
    return result.get("final_summary", "")


# ==================== ç¤ºä¾‹å’Œæµ‹è¯• ====================


async def run_examples():
    """è¿è¡Œç¤ºä¾‹"""
    logger.info("å¼€å§‹è¿è¡ŒWebæœç´¢å¤„ç†ç¤ºä¾‹")

    # ç¤ºä¾‹1: ç½‘é¡µå†…å®¹æ€»ç»“
    try:
        logger.info("ç¤ºä¾‹1: ç½‘é¡µå†…å®¹æ€»ç»“")
        config = ProcessingConfig(token_max=2000, enable_human_review=False, enable_cache=True)

        summary = await summarize_web_content(
            "https://lilianweng.github.io/posts/2023-06-23-agent/", SummaryStrategy.CONCISE, config
        )
        logger.info("ç½‘é¡µæ‘˜è¦ç”ŸæˆæˆåŠŸ", summary_length=len(summary))
        print(f"\nç½‘é¡µæ‘˜è¦:\n{summary}\n")

    except Exception as e:
        logger.exception("ç½‘é¡µå†…å®¹æ€»ç»“å¤±è´¥", exc_info=e)

    # ç¤ºä¾‹2: å¤šç§ç­–ç•¥å¯¹æ¯”
    try:
        logger.info("ç¤ºä¾‹2: å¤šç§ç­–ç•¥å¯¹æ¯”")
        test_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
        ]

        strategies = [SummaryStrategy.CONCISE, SummaryStrategy.DETAILED, SummaryStrategy.BULLET_POINTS]

        for strategy in strategies:
            summary = await summarize_text_list(test_texts, strategy)
            logger.info(f"{strategy.value}ç­–ç•¥æ‘˜è¦ç”ŸæˆæˆåŠŸ", summary_length=len(summary))
            print(f"\n{strategy.value}æ‘˜è¦:\n{summary}\n")

    except Exception as e:
        logger.exception("å¤šç­–ç•¥å¯¹æ¯”å¤±è´¥", exc_info=e)


def visualize_graph():
    """å¯è§†åŒ–å›¾ç»“æ„"""
    try:
        processor = WebSearchProcessor()
        app = processor.create_map_reduce_graph()

        # å°è¯•ç”Ÿæˆå›¾çš„å¯è§†åŒ–
        try:
            from IPython.display import Image, display

            display(Image(app.get_graph().draw_mermaid_png()))
        except ImportError:
            logger.info("IPythonæœªå®‰è£…ï¼Œè·³è¿‡å›¾åƒæ˜¾ç¤º")

        logger.info("ä¼˜åŒ–åçš„å›¾ç»“æ„åŒ…å«ä»¥ä¸‹èŠ‚ç‚¹:")
        logger.info("- generate_summary: ç”Ÿæˆå•ä¸ªæ–‡æ¡£æ‘˜è¦")
        logger.info("- collect_summaries: æ”¶é›†æ‰€æœ‰æ‘˜è¦")
        logger.info("- collapse_summaries: æŠ˜å è¿‡é•¿çš„æ‘˜è¦")
        logger.info("- human_review: äººå·¥å®¡æ ¸èŠ‚ç‚¹")
        logger.info("- generate_final_summary: ç”Ÿæˆæœ€ç»ˆæ‘˜è¦")

    except Exception as e:
        logger.exception("å¯è§†åŒ–å¤±è´¥", exc_info=e)


# ==================== ä¸»å‡½æ•° ====================


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨ä¼˜åŒ–ç‰ˆWebæœç´¢å¤„ç†å™¨")
    logger.info("=" * 50)

    # å¯è§†åŒ–å›¾ç»“æ„
    visualize_graph()

    # è¿è¡Œç¤ºä¾‹
    # await run_examples()


if __name__ == "__main__":
    asyncio.run(main())
