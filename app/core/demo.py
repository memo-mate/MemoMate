import os
import urllib3
import requests
from langchain_community.callbacks.openai_info import OpenAICallbackHandler
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from rich import box, print
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Prompt
from rich.text import Text
from dotenv import load_dotenv
from app.core.vector_search import VectorSearch
from app.core.multi_query import MultiQuerySearch

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ç¦ç”¨ä»£ç†
os.environ["HTTP_PROXY"] = ""
os.environ["HTTPS_PROXY"] = ""

EMPTY_PROXIES = {
    "http": "",
    "https": "",
}


class RichStreamingCallbackHandler(OpenAICallbackHandler):
    """ä½¿ç”¨Richå®ç°æµå¼è¾“å‡ºçš„å›è°ƒå¤„ç†å™¨"""

    def __init__(self, robot_name: str):
        self.console = Console()
        self.robot_name = robot_name
        self.text = ""
        self.live = None

    def on_llm_start(self, *args, **kwargs):
        self.text = ""
        self.live = Live(
            Panel(
                Text("æ€è€ƒä¸­...", style="yellow"),
                title=f"ğŸ¤– {self.robot_name}",
                border_style="cyan",
                box=box.ROUNDED,
            ),
            refresh_per_second=4,
            console=self.console,
        )
        self.live.start()

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.live.update(
            Panel(
                Markdown(self.text),
                title=f"ğŸ¤– {self.robot_name}",
                border_style="cyan",
                box=box.ROUNDED,
            )
        )

    def on_llm_end(self, *args, **kwargs):
        if self.live:
            self.live.stop()


class Assistant:
    def __init__(self, project_name: str, robot_name: str, user: str):
        self.project_name = project_name
        self.robot_name = robot_name
        self.user = user
        # åˆå§‹åŒ–æœ¬åœ°å‘é‡æœç´¢
        self.vector_search = VectorSearch()
        # åˆå§‹åŒ–å¤šæŸ¥è¯¢æ£€ç´¢
        self.multi_query_search = MultiQuerySearch()

        # è®¾ç½®OpenAI APIå‚æ•°
        self.api_base = os.getenv("OPENAI_API_BASE")
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model_name = os.getenv(
            "MODEL_NAME", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        )
        self.temperature = 0.1

    def _get_context(self, query, use_multi_query=False):
        """ä»æœ¬åœ°å‘é‡æ•°æ®åº“è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        if use_multi_query:
            # ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºæ£€ç´¢
            results = self.multi_query_search.search(query, k=10, use_multi_query=True)
        else:
            # ä½¿ç”¨æ™®é€šæ£€ç´¢
            results = self.vector_search.similarity_search(query, k=10)

        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context_texts = []
        for i, (doc, similarity) in enumerate(results):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            page = doc.metadata.get("page", "")
            page_info = f"(ç¬¬{page}é¡µ)" if page else ""

            # æ·»åŠ å·¥ä½œè¡¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            sheet_name = doc.metadata.get("sheet_name", "")
            sheet_info = f"(å·¥ä½œè¡¨: {sheet_name})" if sheet_name else ""

            # æ·»åŠ æ®µè½ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            paragraph = doc.metadata.get("paragraph", "")
            paragraph_info = f"(æ®µè½: {paragraph})" if paragraph else ""

            context_text = f"[æ–‡æ¡£{i+1}] {source}{page_info}{sheet_info}{paragraph_info}\n{doc.page_content}\n"
            context_texts.append(context_text)

        return "\n".join(context_texts)

    def _call_openai_api(self, messages):
        """è°ƒç”¨OpenAI API"""
        # ç¦ç”¨ä»£ç†
        os.environ["HTTP_PROXY"] = ""
        os.environ["HTTPS_PROXY"] = ""

        # æ£€æŸ¥APIé…ç½®
        if not self.api_base:
            raise ValueError("APIåŸºç¡€URLæœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡OPENAI_API_BASE")
        if not self.api_key:
            raise ValueError("APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡OPENAI_API_KEY")

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
        }

        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
        }

        response = requests.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=payload,
            verify=False,  # ç¦ç”¨SSLéªŒè¯ï¼Œä»…ç”¨äºæ¼”ç¤º
            proxies=EMPTY_PROXIES,  # æ˜¾å¼è®¾ç½®ç©ºä»£ç†
        )

        if response.status_code != 200:
            error_msg = f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
            try:
                error_data = response.json()
                if "error" in error_data:
                    error_msg += f" - {error_data['error'].get('message', '')}"
            except Exception:
                error_msg += f" - {response.text[:100]}"
                return error_msg

        return response.json()["choices"][0]["message"]["content"]

    def local_db_agent(self):
        """ä½¿ç”¨æœ¬åœ°æ•°æ®åº“çš„é—®ç­”ä»£ç†"""
        print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = self._get_context(message)

            # æ„å»ºæ¶ˆæ¯
            prompt_template = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
            
            æ£€ç´¢åˆ°çš„ä¿¡æ¯:
            {context}
            
            ç”¨æˆ·é—®é¢˜: {question}
            
            è¯·ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„ä¿¡æ¯æ¥æºã€‚
            """

            prompt = prompt_template.format(context=context, question=message)
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚",
                },
                {"role": "user", "content": prompt},
            ]

            # è°ƒç”¨API
            answer = self._call_openai_api(messages)

            # æ‰“å°å›ç­”
            print(
                f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]{answer}[/bold green] \n"
            )

    def local_db_agent_stream(self):
        """ä½¿ç”¨æœ¬åœ°æ•°æ®åº“çš„æµå¼é—®ç­”ä»£ç†"""
        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        callback_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = self._get_context(message)

            # åˆ›å»ºLLM
            llm = ChatOpenAI(
                model=self.model_name,
                api_key=self.api_key,
                base_url=self.api_base,
                temperature=self.temperature,
                max_tokens=5000,
                timeout=None,
                max_retries=3,
                streaming=True,  # æµå¼è¾“å‡º
                callbacks=[callback_handler],
            )

            # åˆ›å»ºæç¤ºæ¨¡æ¿
            prompt = ChatPromptTemplate.from_template(
                """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
                
                æ£€ç´¢åˆ°çš„ä¿¡æ¯:
                {context}
                
                ç”¨æˆ·é—®é¢˜: {question}
                
                è¯·ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„ä¿¡æ¯æ¥æºã€‚"""
            )

            # è°ƒç”¨LLMè¿›è¡Œæµå¼è¾“å‡º
            llm.invoke(
                prompt.format(context=context, question=message),
                config={"callbacks": [callback_handler]},
            )

    def local_db_agent_with_multi_query(self):
        """ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºçš„æœ¬åœ°æ•°æ®åº“é—®ç­”ä»£ç†"""
        print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            try:
                # è·å–ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºï¼‰
                print("[yellow]æ­£åœ¨ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºæ£€ç´¢...[/yellow]")
                context = self._get_context(message, use_multi_query=True)

                # æ„å»ºæ¶ˆæ¯
                prompt_template = """
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
                
                æ£€ç´¢åˆ°çš„ä¿¡æ¯:
                {context}
                
                ç”¨æˆ·é—®é¢˜: {question}
                
                è¯·ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„ä¿¡æ¯æ¥æºã€‚
                """

                prompt = prompt_template.format(context=context, question=message)
                messages = [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ]

                # è°ƒç”¨API
                answer = self._call_openai_api(messages)

                # æ‰“å°å›ç­”
                print(
                    f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]{answer}[/bold green] \n"
                )
            except Exception as e:
                print(f"[bold red]å‘ç”Ÿé”™è¯¯: {str(e)}[/bold red]")
                print("[bold yellow]è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®[/bold yellow]")

    def local_db_agent_with_multi_query_stream(self):
        """ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºçš„æµå¼æœ¬åœ°æ•°æ®åº“é—®ç­”ä»£ç†"""
        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        callback_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            try:
                # è·å–ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºï¼‰
                print("[yellow]æ­£åœ¨ä½¿ç”¨å¤šæŸ¥è¯¢å¢å¼ºæ£€ç´¢...[/yellow]")
                context = self._get_context(message, use_multi_query=True)

                try:
                    # åˆ›å»ºLLM
                    llm = ChatOpenAI(
                        model=self.model_name,
                        api_key=self.api_key,
                        base_url=self.api_base,
                        temperature=self.temperature,
                        max_tokens=3200,
                        timeout=60,  # å¢åŠ è¶…æ—¶æ—¶é—´
                        max_retries=5,  # å¢åŠ é‡è¯•æ¬¡æ•°
                        streaming=True,  # æµå¼è¾“å‡º
                        callbacks=[callback_handler],
                    )

                    # åˆ›å»ºæç¤ºæ¨¡æ¿
                    prompt = ChatPromptTemplate.from_template(
                        """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                        å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
                        
                        æ£€ç´¢åˆ°çš„ä¿¡æ¯:
                        {context}
                        
                        ç”¨æˆ·é—®é¢˜: {question}
                        
                        è¯·ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„ä¿¡æ¯æ¥æºã€‚"""
                    )

                    # è°ƒç”¨LLMè¿›è¡Œæµå¼è¾“å‡º
                    llm.invoke(
                        prompt.format(context=context, question=message),
                        config={"callbacks": [callback_handler]},
                    )
                except Exception as api_err:
                    print(
                        f"[red]æµå¼APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨éæµå¼API: {str(api_err)}[/red]"
                    )

                    # æ„å»ºæ¶ˆæ¯
                    prompt_template = """
                    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                    å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯æ— æ³•å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
                    
                    æ£€ç´¢åˆ°çš„ä¿¡æ¯:
                    {context}
                    
                    ç”¨æˆ·é—®é¢˜: {question}
                    
                    è¯·ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„ä¿¡æ¯æ¥æºã€‚
                    """

                    prompt = prompt_template.format(context=context, question=message)
                    messages = [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚",
                        },
                        {"role": "user", "content": prompt},
                    ]

                    # ä½¿ç”¨éæµå¼APIè°ƒç”¨
                    print("[yellow]æ­£åœ¨ä½¿ç”¨éæµå¼APIè°ƒç”¨...[/yellow]")
                    answer = self._call_openai_api(messages)

                    # æ‰“å°å›ç­”
                    print(
                        f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]{answer}[/bold green] \n"
                    )
            except Exception as e:
                print(f"[bold red]å‘ç”Ÿé”™è¯¯: {str(e)}[/bold red]")
                print("[bold yellow]è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®[/bold yellow]")

    def demo_run(self):
        print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break
            print(
                f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]{message}[/bold green] \n"
            )


if __name__ == "__main__":
    assistant = Assistant(project_name="æœ¬åœ°çŸ¥è¯†åº“", robot_name="å°åŠ©æ‰‹", user="ç”¨æˆ·")

    # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å¼
    print("[bold]è¯·é€‰æ‹©ä½¿ç”¨æ¨¡å¼:[/bold]")
    print("1. åŸºæœ¬æ£€ç´¢æ¨¡å¼")
    print("2. å¤šæŸ¥è¯¢å¢å¼ºæ£€ç´¢æ¨¡å¼")
    print("3. å¤šæŸ¥è¯¢å¢å¼ºæµå¼æ¨¡å¼")
    print("4. åŸºæœ¬æµå¼æ¨¡å¼")
    print("5. æœ¬åœ°æ¨¡å‹æ¨¡å¼")

    choice = Prompt.ask("è¯·è¾“å…¥é€‰é¡¹", choices=["1", "2", "3", "4", "5"], default="3")

    if choice == "1":
        assistant.local_db_agent()
    elif choice == "2":
        assistant.local_db_agent_with_multi_query()
    elif choice == "3":
        assistant.local_db_agent_with_multi_query_stream()
    elif choice == "4":
        assistant.local_db_agent_stream()
    elif choice == "5":
        assistant.local_db_agent_with_local_model()
