import os
import tempfile

import requests
import rich
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import MarkdownTextSplitter
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.progress import BarColumn, MofNCompleteColumn, Progress, TaskProgressColumn, TimeElapsedColumn
from rich.prompt import Prompt

from app.configs import settings
from app.models.custom_retriever import MULTI_QUERY_PROMPT, get_custom_retriever
from app.utils.stream_handler import RichStreamingCallbackHandler


class Assistant:
    def __init__(self, project_name: str, robot_name: str, user: str):
        self.project_name = project_name
        self.robot_name = robot_name
        self.user = user

    def create_knowledge_base(self, pdf_url: str, collection_name: str, recreate: bool = False):
        """åˆ›å»ºçŸ¥è¯†åº“"""
        # ä¸‹è½½PDFåˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.gettempdir()
        pdf_path = os.path.join(temp_dir, "document.pdf")

        # ä»…å½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–éœ€è¦é‡å»ºæ—¶ä¸‹è½½
        if not os.path.exists(pdf_path) or recreate:
            response = requests.get(pdf_url)
            with open(pdf_path, "wb") as f:
                f.write(response.content)

        # åŠ è½½PDFå¹¶åˆ†å‰²æ–‡æ¡£
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
        splits = text_splitter.split_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings(
            model_name="./bge-m3",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "mps"},
        )
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=f"./chroma_db_{collection_name}",
        )

        return vectordb

    def get_vector_store(self, collection_name: str = "recipes", is_exist: bool = False) -> Chroma:
        """è·å–å‘é‡å­˜å‚¨"""
        # ç»ˆç«¯è¾“å‡º
        console = Console()
        with console.status("[bold green]æ­£åœ¨åŠ è½½embeddingæ¨¡å‹...[/bold green]", spinner="bouncingBall"):
            # åˆ›å»ºå‘é‡å­˜å‚¨
            embeddings = HuggingFaceEmbeddings(
                model_name="./bge-m3",
                encode_kwargs={"normalize_embeddings": True},
                model_kwargs={"device": "mps"},
            )

        title = "æ­£åœ¨åŠ è½½å‘é‡å­˜å‚¨..." if is_exist else "æ­£åœ¨åˆ›å»ºå‘é‡å­˜å‚¨..."
        with console.status(f"[bold green]{title}[/bold green]", spinner="bouncingBall"):
            vectordb = Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=f"./chroma_db_{collection_name}",
            )
        return vectordb

    def load_local_md_files(self, collection_name: str = "recipes"):
        """åŠ è½½æœ¬åœ°MDæ–‡ä»¶"""
        # ç»ˆç«¯è¾“å‡º
        console = Console()
        is_exist = os.path.exists(f"./chroma_db_{collection_name}")
        # æ‰“å°æç¤ºï¼Œåˆæ¬¡åŠ è½½ï¼Œåˆå§‹åŒ–å‘é‡å­˜å‚¨
        vectordb = self.get_vector_store(collection_name, is_exist=is_exist)
        if not is_exist:
            console.print("[bold green]åˆæ¬¡åŠ è½½ï¼Œåˆå§‹åŒ–çŸ¥è¯†åº“...[/bold green]")
            with console.status("[bold green]æ­£åœ¨åŠ è½½æœ¬åœ°MDæ–‡ä»¶...[/bold green]", spinner="bouncingBall"):
                dir_path = "./data/Miner2PdfAndWord_Markitdown2Excel"
                loader = DirectoryLoader(dir_path, glob="**/*.md")
                documents = loader.load()
                if not documents:
                    raise ValueError("æ–‡æ¡£ç›®å½•ä¸ºç©º")
            with console.status("[bold green]æ­£åœ¨åˆ†å‰²æ–‡æ¡£...[/bold green]", spinner="bouncingBall"):
                # ä½¿ç”¨ MarkdownTextSplitter åˆ†å‰²æ–‡æ¡£ï¼Œchunk å¤§å°ä¸º 3200ï¼Œé‡å  30
                text_spliter = MarkdownTextSplitter(
                    chunk_size=settings.chunk_size, chunk_overlap=settings.chunk_overlap
                )
                # åˆ†å‰²æ–‡æ¡£
                split_docs = text_spliter.split_documents(documents)

            overall_progress = Progress(
                "[progress.description]{task.description}",
                BarColumn(),
                MofNCompleteColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                # "[progress.percentage]{task.percentage:>3.0f}%",
                # "[progress.completed]{task.completed}",
                # "[progress.total]{task.total}",
            )
            overall_task = overall_progress.add_task("åˆ›å»ºçŸ¥è¯†åº“", total=len(split_docs))
            progress_panel = Panel.fit(overall_progress, title="åµŒå…¥æ–‡æ¡£", border_style="green")

            with Live(progress_panel, refresh_per_second=10):
                for doc in split_docs:
                    vectordb.add_documents([doc])
                    overall_progress.advance(overall_task)

        return vectordb

    def pdf_agent_stream(self, user: str = "user"):
        # åˆ›å»ºçŸ¥è¯†åº“
        # pdf_url = "https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        # vectordb = self.create_knowledge_base(pdf_url, "recipes")
        vectordb = self.load_local_md_files()
        return
        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        streaming_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        # åˆ›å»ºLLM
        llm = ChatOpenAI(
            model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            # model="Qwen/QwQ-32B",
            api_key=settings.openai_api_key,
            base_url="https://api.siliconflow.cn/v1",
            temperature=0,
            max_tokens=3200,
            timeout=None,
            max_retries=3,
            streaming=True,  # æµå¼è¾“å‡º
            stream_usage=True,
            # callbacks=[streaming_handler],
        )

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_template(
            """å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡: {context}
é—®é¢˜: {question}

å›ç­”:"""
        )

        # åˆ›å»ºæ–‡æ¡£é“¾å’Œæ£€ç´¢é“¾
        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        query_retriever = get_custom_retriever(llm, vectordb)

        result_chain = query_retriever | question_answer_chain
        # retrieval_chain = create_retrieval_chain(retriever, question_answer_chain)

        # äº¤äº’å¾ªç¯
        rich.print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æ¨¡å‹åç§°: {llm.model_name}[/bold green] \n")
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æœ€å¤§ä¸Šä¸‹æ–‡: {llm.max_tokens}[/bold green] \n")

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨
            print(f"prompt: {MULTI_QUERY_PROMPT.format(question=message)}")
            # è°ƒç”¨é“¾è¿›è¡Œæµå¼è¾“å‡º - ä¿®æ”¹è¿™é‡Œ
            result_chain.invoke({"question": message}, config={"callbacks": [streaming_handler]})
            # result_chain.invoke({"question": message})

            rich.print("ğŸ¬ é€€å‡º")
            break


if __name__ == "__main__":
    assistant = Assistant(project_name="é­…é­”éª‚ä»–", robot_name="å°é­…", user="daoji")
    assistant.pdf_agent_stream()
