import os
import tempfile

import requests
import rich
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import MarkdownTextSplitter
from rich.prompt import Prompt

from app.configs import settings
from app.models.custom_retriever import MULTI_QUERY_PROMPT, get_custom_retriever
from app.utils.stream_handler import RichStreamingCallbackHandler


class Assistant:
    def __init__(self, project_name: str, robot_name: str, user: str):
        self.project_name = project_name
        self.robot_name = robot_name
        self.user = user

    def create_knowledge_base(self, pdf_url: str, collection_name: str, recreate: bool = False):
        """åˆ›å»ºçŸ¥è¯†åº“"""
        # ä¸‹è½½PDFåˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.gettempdir()
        pdf_path = os.path.join(temp_dir, "document.pdf")

        # ä»…å½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–éœ€è¦é‡å»ºæ—¶ä¸‹è½½
        if not os.path.exists(pdf_path) or recreate:
            response = requests.get(pdf_url)
            with open(pdf_path, "wb") as f:
                f.write(response.content)

        # åŠ è½½PDFå¹¶åˆ†å‰²æ–‡æ¡£
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
        splits = text_splitter.split_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings(
            model_name="./bge-m3",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "mps"},
        )
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=f"./chroma_db_{collection_name}",
        )

        return vectordb

    def load_local_md_files(self, collection_name: str = "recipes"):
        """åŠ è½½æœ¬åœ°MDæ–‡ä»¶"""
        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings(
            model_name="./bge-m3",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "mps"},
        )

        # å¦‚æœå­˜åœ¨å‘é‡å­˜å‚¨ï¼Œåˆ™ç›´æ¥è¿”å›
        if os.path.exists(f"./chroma_db_{collection_name}"):
            return Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=f"./chroma_db_{collection_name}",
            )

        dir_path = "./data/Miner2PdfAndWord_Markitdown2Excel"
        loader = DirectoryLoader(dir_path, glob="**/*.md")
        documents = loader.load()
        if not documents:
            raise ValueError("æ–‡æ¡£ç›®å½•ä¸ºç©º")
        # ä½¿ç”¨ MarkdownTextSplitter åˆ†å‰²æ–‡æ¡£ï¼Œchunk å¤§å°ä¸º 3200ï¼Œé‡å  30
        text_spliter = MarkdownTextSplitter(chunk_size=settings.chunk_size, chunk_overlap=settings.chunk_overlap)

        # åˆ†å‰²æ–‡æ¡£
        split_docs = text_spliter.split_documents(documents)

        vectordb = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=f"./chroma_db_{collection_name}",
        )

        return vectordb

    def pdf_agent_stream(self, user: str = "user"):
        # åˆ›å»ºçŸ¥è¯†åº“
        # pdf_url = "https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        # vectordb = self.create_knowledge_base(pdf_url, "recipes")
        vectordb = self.load_local_md_files()

        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        streaming_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        # åˆ›å»ºLLM
        llm = ChatOpenAI(
            model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            # model="Qwen/QwQ-32B",
            api_key=settings.openai_api_key,
            base_url="https://api.siliconflow.cn/v1",
            temperature=0,
            max_tokens=3200,
            timeout=None,
            max_retries=3,
            streaming=True,  # æµå¼è¾“å‡º
            stream_usage=True,
            # callbacks=[streaming_handler],
        )

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_template(
            """å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡: {context}
é—®é¢˜: {question}

å›ç­”:"""
        )

        # åˆ›å»ºæ–‡æ¡£é“¾å’Œæ£€ç´¢é“¾
        question_answer_chain = create_stuff_documents_chain(llm, prompt)
        query_retriever = get_custom_retriever(llm, vectordb)

        result_chain = query_retriever | question_answer_chain
        # retrieval_chain = create_retrieval_chain(retriever, question_answer_chain)

        # äº¤äº’å¾ªç¯
        rich.print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æ¨¡å‹åç§°: {llm.model_name}[/bold green] \n")
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æœ€å¤§ä¸Šä¸‹æ–‡: {llm.max_tokens}[/bold green] \n")

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨
            print(f"prompt: {MULTI_QUERY_PROMPT.format(question=message)}")
            # è°ƒç”¨é“¾è¿›è¡Œæµå¼è¾“å‡º - ä¿®æ”¹è¿™é‡Œ
            result_chain.invoke({"question": message}, config={"callbacks": [streaming_handler]})
            # result_chain.invoke({"question": message})

            rich.print("ğŸ¬ é€€å‡º")
            break


if __name__ == "__main__":
    assistant = Assistant(project_name="é­…é­”éª‚ä»–", robot_name="å°é­…", user="daoji")
    assistant.pdf_agent_stream()
