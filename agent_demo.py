import os
import tempfile

import requests
import rich
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_core.output_parsers import BaseOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import MarkdownTextSplitter
from rich.prompt import Prompt

from app.configs import settings
from app.utils.stream_handler import RichStreamingCallbackHandler


class Assistant:
    def __init__(self, project_name: str, robot_name: str, user: str):
        self.project_name = project_name
        self.robot_name = robot_name
        self.user = user

    def create_knowledge_base(self, pdf_url: str, collection_name: str, recreate: bool = False):
        """åˆ›å»ºçŸ¥è¯†åº“"""
        # ä¸‹è½½PDFåˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.gettempdir()
        pdf_path = os.path.join(temp_dir, "document.pdf")

        # ä»…å½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–éœ€è¦é‡å»ºæ—¶ä¸‹è½½
        if not os.path.exists(pdf_path) or recreate:
            response = requests.get(pdf_url)
            with open(pdf_path, "wb") as f:
                f.write(response.content)

        # åŠ è½½PDFå¹¶åˆ†å‰²æ–‡æ¡£
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=50)
        splits = text_splitter.split_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings(
            model_name="./bge-m3",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "mps"},
        )
        vectordb = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=f"./chroma_db_{collection_name}",
        )

        return vectordb

    def load_local_md_files(self, collection_name: str = "recipes"):
        """åŠ è½½æœ¬åœ°MDæ–‡ä»¶"""
        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = HuggingFaceEmbeddings(
            model_name="./bge-m3",
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "mps"},
        )

        # å¦‚æœå­˜åœ¨å‘é‡å­˜å‚¨ï¼Œåˆ™ç›´æ¥è¿”å›
        if os.path.exists(f"./chroma_db_{collection_name}"):
            return Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=f"./chroma_db_{collection_name}",
            )

        dir_path = "./data/Miner2PdfAndWord_Markitdown2Excel"
        loader = DirectoryLoader(dir_path, glob="**/*.md")
        documents = loader.load()
        if not documents:
            raise ValueError("æ–‡æ¡£ç›®å½•ä¸ºç©º")
        # ä½¿ç”¨ MarkdownTextSplitter åˆ†å‰²æ–‡æ¡£ï¼Œchunk å¤§å°ä¸º 3200ï¼Œé‡å  30
        text_spliter = MarkdownTextSplitter(chunk_size=settings.chunk_size, chunk_overlap=settings.chunk_overlap)

        # åˆ†å‰²æ–‡æ¡£
        split_docs = text_spliter.split_documents(documents)

        vectordb = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=f"./chroma_db_{collection_name}",
        )

        return vectordb

    def pdf_agent_stream(self, user: str = "user"):
        # åˆ›å»ºçŸ¥è¯†åº“
        # pdf_url = "https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        # vectordb = self.create_knowledge_base(pdf_url, "recipes")
        vectordb = self.load_local_md_files()

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = vectordb.as_retriever(search_kwargs={"k": 3})

        # åˆ›å»ºå›è°ƒå¤„ç†å™¨
        streaming_handler = RichStreamingCallbackHandler(robot_name=self.robot_name)

        # åˆ›å»ºLLM
        llm = ChatOpenAI(
            # model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            model="Qwen/QwQ-32B",
            api_key=settings.openai_api_key,
            base_url="https://api.siliconflow.cn/v1",
            temperature=0,
            max_tokens=3200,
            timeout=None,
            max_retries=3,
            streaming=True,  # æµå¼è¾“å‡º
            callbacks=[streaming_handler],
        )

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        #     prompt = ChatPromptTemplate.from_template(
        #         """å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

        # ä¸Šä¸‹æ–‡: {context}
        # é—®é¢˜: {input}

        # å›ç­”:"""
        #     )

        # åˆ›å»ºæ–‡æ¡£é“¾å’Œæ£€ç´¢é“¾
        # question_answer_chain = create_stuff_documents_chain(llm, prompt)
        # retrieval_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Output parser will split the LLM result into a list of queries
        class LineListOutputParser(BaseOutputParser[list[str]]):
            """Output parser for a list of lines."""

            def parse(self, text: str) -> list[str]:
                lines = text.strip().split("\n")
                return list(filter(None, lines))  # Remove empty lines

        output_parser = LineListOutputParser()

        QUERY_PROMPT = PromptTemplate.from_template(
            template="""ä½ æ˜¯ä¸€ä¸ªaiè¯­è¨€æ¨¡å‹åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆäº”ä¸ªä¸åŒçš„ç‰ˆæœ¬çš„ç”¨æˆ·é—®é¢˜ï¼Œä»¥ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚é€šè¿‡ç”Ÿæˆç”¨æˆ·é—®é¢˜çš„å¤šä¸ªè§†è§’ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å…‹æœåŸºäºè·ç¦»çš„ç›¸ä¼¼åº¦æœç´¢çš„ä¸€äº›å±€é™æ€§ã€‚
ä»¥ä¸‹ä¸ºæ›¿ä»£é—®é¢˜ï¼Œ ä»¥æ¢è¡Œç¬¦åˆ†éš”ï¼Œä¸è¦è¾“å‡ºæ¨ç†è¿‡ç¨‹ã€‚
åŸå§‹é—®é¢˜ï¼š{question}""",
        )

        llm_chain = QUERY_PROMPT | llm | output_parser
        # åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨
        query_retriever = MultiQueryRetriever(retriever=retriever, llm_chain=llm_chain, parser_key="lines")

        # äº¤äº’å¾ªç¯
        rich.print(
            f"\n[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]ä½ å¥½ï¼Œæˆ‘æ˜¯{self.project_name}çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘{self.robot_name}ã€‚"
            "è¾“å…¥[bold yellow] exit[/bold yellow] æˆ– [bold yellow]bye[/bold yellow] é€€å‡ºã€‚\n"
        )
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æ¨¡å‹åç§°: {llm.model_name}[/bold green] \n")
        rich.print(f"[bold cyan] ğŸ¤– AI:[/bold cyan] [bold green]æœ€å¤§ä¸Šä¸‹æ–‡: {llm.max_tokens}[/bold green] \n")

        while True:
            message = Prompt.ask(f"[bold] :sunglasses: {self.user}[/bold]")
            if message in ("exit", "bye"):
                break

            # è°ƒç”¨é“¾è¿›è¡Œæµå¼è¾“å‡º - ä¿®æ”¹è¿™é‡Œ
            # retrieval_chain.invoke({"input": message}, config={"callbacks": [streaming_handler]})
            # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨
            print(f"prompt: {QUERY_PROMPT.format(question=message)}")
            docs = query_retriever.invoke(message)
            rich.inspect(docs)
            rich.print("ğŸ¬ é€€å‡º")
            break


if __name__ == "__main__":
    assistant = Assistant(project_name="é­…é­”éª‚ä»–", robot_name="å°é­…", user="daoji")
    assistant.pdf_agent_stream()
