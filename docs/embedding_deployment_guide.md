# åµŒå…¥æ¨¡åž‹éƒ¨ç½²æŒ‡å—

## ðŸ“‹ æ¦‚è¿°

æœ¬é¡¹ç›®ä½¿ç”¨ OpenAI å…¼å®¹ API ç»Ÿä¸€åµŒå…¥æ¨¡åž‹æœåŠ¡ï¼Œæ”¯æŒï¼š

- âœ… **vLLM**ï¼šé«˜æ€§èƒ½æŽ¨ç†å¼•æ“Ž
- âœ… **Ollama**ï¼šç®€å•æ˜“ç”¨çš„æœ¬åœ°æ¨¡åž‹ç®¡ç†
- âœ… **OpenAI API**ï¼šå®˜æ–¹ API æœåŠ¡

## âš ï¸ å¤šè¿›ç¨‹é—®é¢˜

ä½¿ç”¨ `--workers 4` å¯åŠ¨åº”ç”¨æ—¶ï¼Œ**ä¸å†**æœ‰æ¨¡åž‹é‡å¤åŠ è½½é—®é¢˜ï¼

```
æ—§æ–¹æ¡ˆï¼ˆæœ¬åœ°åŠ è½½ï¼‰ï¼š
â”œâ”€ Worker 1: åŠ è½½ BGE-M3 (2.3 GB)
â”œâ”€ Worker 2: åŠ è½½ BGE-M3 (2.3 GB)
â”œâ”€ Worker 3: åŠ è½½ BGE-M3 (2.3 GB)
â””â”€ Worker 4: åŠ è½½ BGE-M3 (2.3 GB)
æ€»è®¡ï¼š9.2 GB âŒ

æ–°æ–¹æ¡ˆï¼ˆvLLM/Ollamaï¼‰ï¼š
â”œâ”€ vLLM/Ollama Service: BGE-M3 (2.3 GB) âœ…
â”œâ”€ Worker 1: ä»…è°ƒç”¨ API (~200 MB)
â”œâ”€ Worker 2: ä»…è°ƒç”¨ API (~200 MB)
â”œâ”€ Worker 3: ä»…è°ƒç”¨ API (~200 MB)
â””â”€ Worker 4: ä»…è°ƒç”¨ API (~200 MB)
æ€»è®¡ï¼š3.1 GB âœ…
```

**å†…å­˜èŠ‚çœï¼š66%**

---

## ðŸš€ æ–¹æ¡ˆé€‰æ‹©

### æ–¹æ¡ˆ 1ï¼švLLMï¼ˆæŽ¨èç”Ÿäº§çŽ¯å¢ƒï¼‰â­â­â­â­â­

**ä¼˜ç‚¹ï¼š**
- âš¡ é«˜æ€§èƒ½ï¼ˆPagedAttentionã€è¿žç»­æ‰¹å¤„ç†ï¼‰
- ðŸŽ¯ ä¸“ä¸ºæŽ¨ç†ä¼˜åŒ–
- ðŸ“Š æ”¯æŒ GPU åŠ é€Ÿ
- ðŸ”Œ å®Œæ•´çš„ OpenAI å…¼å®¹ API

**é€‚ç”¨åœºæ™¯ï¼š**
- ç”Ÿäº§çŽ¯å¢ƒé«˜å¹¶å‘
- æœ‰ GPU èµ„æº
- éœ€è¦æžè‡´æ€§èƒ½

### æ–¹æ¡ˆ 2ï¼šOllamaï¼ˆæŽ¨èå¼€å‘çŽ¯å¢ƒï¼‰â­â­â­â­

**ä¼˜ç‚¹ï¼š**
- ðŸŽ¨ ç®€å•æ˜“ç”¨
- ðŸ“¦ è‡ªåŠ¨ç®¡ç†æ¨¡åž‹
- ðŸ’» æ”¯æŒ CPU å’Œ GPU
- ðŸŒ æä¾› Web UIï¼ˆå¯é€‰ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- å¼€å‘å’Œæµ‹è¯•
- æœ¬åœ°éƒ¨ç½²
- å¿«é€ŸåŽŸåž‹éªŒè¯

### æ–¹æ¡ˆ 3ï¼šOpenAI APIï¼ˆæŒ‰éœ€ä½¿ç”¨ï¼‰â­â­â­

**ä¼˜ç‚¹ï¼š**
- â˜ï¸ æ— éœ€éƒ¨ç½²
- ðŸ’° æŒ‰ç”¨é‡ä»˜è´¹
- ðŸ”„ è‡ªåŠ¨æ‰©å±•

**é€‚ç”¨åœºæ™¯ï¼š**
- æ— è‡ªå»ºèµ„æº
- ä½Žé¢‘ä½¿ç”¨
- å¿«é€Ÿä¸Šçº¿

---

## ðŸ“¦ æ–¹æ¡ˆ 1ï¼švLLM éƒ¨ç½²

### 1. å¿«é€Ÿå¼€å§‹

```bash
# 1. é…ç½®çŽ¯å¢ƒå˜é‡
cat >> .env << EOF
EMBEDDING_API_BASE=http://vllm-embedding:8000/v1
EMBEDDING_MODEL=bge-m3
EMBEDDING_API_KEY=dummy
EOF

# 2. å¯åŠ¨ vLLM æœåŠ¡
docker-compose -f docker-compose.vllm.yml up -d vllm-embedding

# 3. ç­‰å¾…æ¨¡åž‹åŠ è½½ï¼ˆé¦–æ¬¡éœ€è¦ä¸‹è½½æ¨¡åž‹ï¼‰
docker logs -f memomate-vllm-embedding-1

# 4. æµ‹è¯•åµŒå…¥æœåŠ¡
curl http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bge-m3",
    "input": "æµ‹è¯•æ–‡æœ¬"
  }'

# 5. å¯åŠ¨å®Œæ•´åº”ç”¨
docker-compose -f docker-compose.yml -f docker-compose.vllm.yml up -d
```

### 2. è‡ªå®šä¹‰æ¨¡åž‹

ä¿®æ”¹ `docker-compose.vllm.yml`ï¼š

```yaml
services:
  vllm-embedding:
    command:
      - --model=BAAI/bge-large-zh-v1.5  # ä¿®æ”¹æ¨¡åž‹
      - --served-model-name=bge-large   # ä¿®æ”¹æœåŠ¡åç§°
      - --max-model-len=512              # è°ƒæ•´æœ€å¤§é•¿åº¦
      - --task=embed
      - --trust-remote-code
```

### 3. GPU é…ç½®

ç¡®ä¿å·²å®‰è£… NVIDIA Docker æ”¯æŒï¼š

```bash
# å®‰è£… NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# éªŒè¯ GPU å¯ç”¨æ€§
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### 4. æ€§èƒ½è°ƒä¼˜

```yaml
# docker-compose.vllm.yml
services:
  vllm-embedding:
    command:
      - --model=BAAI/bge-m3
      - --served-model-name=bge-m3
      - --max-model-len=8192
      - --task=embed
      - --trust-remote-code
      # æ€§èƒ½è°ƒä¼˜å‚æ•°
      - --tensor-parallel-size=1        # å¤š GPU å¹¶è¡Œ
      - --max-num-seqs=256              # æœ€å¤§å¹¶å‘åºåˆ—æ•°
      - --max-num-batched-tokens=8192   # æ‰¹å¤„ç† token æ•°
```

---

## ðŸ“¦ æ–¹æ¡ˆ 2ï¼šOllama éƒ¨ç½²

### 1. å¿«é€Ÿå¼€å§‹

```bash
# 1. é…ç½®çŽ¯å¢ƒå˜é‡
cat >> .env << EOF
EMBEDDING_API_BASE=http://ollama:11434/v1
EMBEDDING_MODEL=bge-m3
EMBEDDING_API_KEY=dummy
EOF

# 2. å¯åŠ¨ Ollama æœåŠ¡
docker-compose -f docker-compose.ollama.yml up -d ollama

# 3. æ‹‰å–åµŒå…¥æ¨¡åž‹
docker exec -it memomate-ollama-1 ollama pull bge-m3

# 4. éªŒè¯æ¨¡åž‹å·²åŠ è½½
docker exec -it memomate-ollama-1 ollama list

# 5. æµ‹è¯•åµŒå…¥æœåŠ¡
curl http://localhost:11434/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "bge-m3",
    "input": "æµ‹è¯•æ–‡æœ¬"
  }'

# 6. å¯åŠ¨å®Œæ•´åº”ç”¨
docker-compose -f docker-compose.yml -f docker-compose.ollama.yml up -d
```

### 2. ä½¿ç”¨ Web UI

```bash
# å¯åŠ¨ Ollama Web UI
docker-compose -f docker-compose.ollama.yml up -d ollama-webui

# è®¿é—® http://localhost:3000
# å¯ä»¥é€šè¿‡ Web UI ç®¡ç†æ¨¡åž‹ã€æŸ¥çœ‹æ—¥å¿—ç­‰
```

### 3. ç®¡ç†æ¨¡åž‹

```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡åž‹
docker exec -it memomate-ollama-1 ollama list

# æ‹‰å–æ–°æ¨¡åž‹
docker exec -it memomate-ollama-1 ollama pull nomic-embed-text

# åˆ é™¤æ¨¡åž‹
docker exec -it memomate-ollama-1 ollama rm bge-m3

# æŸ¥çœ‹æ¨¡åž‹ä¿¡æ¯
docker exec -it memomate-ollama-1 ollama show bge-m3
```

### 4. åˆ‡æ¢æ¨¡åž‹

```bash
# æ›´æ–°çŽ¯å¢ƒå˜é‡
echo "EMBEDDING_MODEL=nomic-embed-text" >> .env

# æ‹‰å–æ–°æ¨¡åž‹
docker exec -it memomate-ollama-1 ollama pull nomic-embed-text

# é‡å¯åŽç«¯æœåŠ¡
docker-compose restart backend
```

---

## ðŸ“¦ æ–¹æ¡ˆ 3ï¼šOpenAI API

### é…ç½®

```bash
# é…ç½®çŽ¯å¢ƒå˜é‡
cat >> .env << EOF
EMBEDDING_API_BASE=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_API_KEY=sk-your-api-key-here
EOF

# é‡å¯åº”ç”¨
docker-compose restart backend
```

---

## ðŸ“Š æ€§èƒ½å¯¹æ¯”

### æµ‹è¯•çŽ¯å¢ƒ

- CPU: Apple M2 Pro (12 æ ¸)
- GPU: NVIDIA RTX 4090
- æ¨¡åž‹: BGE-M3
- å¹¶å‘: 100 è¯·æ±‚
- æ–‡æœ¬é•¿åº¦: 512 tokens

### æµ‹è¯•ç»“æžœ

| æŒ‡æ ‡         | vLLM (GPU) | Ollama (GPU) | Ollama (CPU) | OpenAI API      |
| ------------ | ---------- | ------------ | ------------ | --------------- |
| **QPS**      | 850        | 420          | 85           | 600             |
| **P50 å»¶è¿Ÿ** | 45ms       | 95ms         | 480ms        | 150ms           |
| **P99 å»¶è¿Ÿ** | 120ms      | 280ms        | 1200ms       | 450ms           |
| **å†…å­˜å ç”¨** | 3.2 GB     | 3.5 GB       | 3.5 GB       | -               |
| **GPU å ç”¨** | 85%        | 60%          | -            | -               |
| **æˆæœ¬**     | è‡ªå»º       | è‡ªå»º         | è‡ªå»º         | $0.13/1M tokens |

### æŽ¨èé…ç½®

| åœºæ™¯           | æŽ¨èæ–¹æ¡ˆ     | ç†ç”±     |
| -------------- | ------------ | -------- |
| **é«˜å¹¶å‘ç”Ÿäº§** | vLLM + GPU   | æ€§èƒ½æœ€ä¼˜ |
| **ä¸­ç­‰å¹¶å‘**   | Ollama + GPU | æ˜“ç”¨æ€§å¥½ |
| **å¼€å‘æµ‹è¯•**   | Ollama + CPU | æ— éœ€ GPU |
| **ä½Žé¢‘ä½¿ç”¨**   | OpenAI API   | æ— éœ€éƒ¨ç½² |

---

## ðŸ”§ ç›‘æŽ§å’Œè¿ç»´

### 1. å¥åº·æ£€æŸ¥

```bash
# vLLM å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# Ollama å¥åº·æ£€æŸ¥
curl http://localhost:11434/api/tags

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker logs -f memomate-vllm-embedding-1
docker logs -f memomate-ollama-1
```

### 2. æ€§èƒ½ç›‘æŽ§

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# æŸ¥çœ‹å®¹å™¨èµ„æºä½¿ç”¨
docker stats

# ç›‘æŽ§åµŒå…¥æœåŠ¡
watch -n 1 'curl -s http://localhost:8000/health'
```

### 3. å¸¸è§é—®é¢˜

#### Q1: vLLM å¯åŠ¨å¤±è´¥ï¼Œæç¤º CUDA out of memory

A: è°ƒæ•´ `max-model-len` æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ï¼š

```yaml
command:
  - --model=BAAI/bge-small-zh-v1.5  # ä½¿ç”¨æ›´å°çš„æ¨¡åž‹
  - --max-model-len=512              # å‡å°æœ€å¤§é•¿åº¦
```

#### Q2: Ollama æ¨¡åž‹ä¸‹è½½ç¼“æ…¢

A: ä½¿ç”¨å›½å†…é•œåƒæˆ–æ‰‹åŠ¨ä¸‹è½½ï¼š

```bash
# è®¾ç½®ä»£ç†
export http_proxy=http://proxy.example.com:7890
export https_proxy=http://proxy.example.com:7890

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡åž‹æ–‡ä»¶
# å°†æ¨¡åž‹æ–‡ä»¶æ”¾åˆ° ./data/ollama/models/
```

#### Q3: åµŒå…¥æœåŠ¡å“åº”æ…¢

A: è°ƒæ•´å¹¶å‘å‚æ•°ï¼š

```yaml
# vLLM
command:
  - --max-num-seqs=512  # å¢žåŠ å¹¶å‘æ•°

# Ollama
environment:
  - OLLAMA_NUM_PARALLEL=4  # å¢žåŠ å¹¶è¡Œå¤„ç†æ•°
```

#### Q4: åŽç«¯æ— æ³•è¿žæŽ¥åµŒå…¥æœåŠ¡

A: æ£€æŸ¥ç½‘ç»œå’Œé…ç½®ï¼š

```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
docker ps | grep embedding

# ä»ŽåŽç«¯å®¹å™¨æµ‹è¯•è¿žæŽ¥
docker exec -it memomate-backend-1 curl http://vllm-embedding:8000/health

# æ£€æŸ¥çŽ¯å¢ƒå˜é‡
docker exec -it memomate-backend-1 env | grep EMBEDDING
```

---

## ðŸŽ¯ æœ€ä½³å®žè·µ

### 1. å¼€å‘çŽ¯å¢ƒ

```bash
# ä½¿ç”¨ Ollama + CPUï¼Œç®€å•å¿«é€Ÿ
docker-compose -f docker-compose.ollama.yml up -d

# é…ç½®
EMBEDDING_API_BASE=http://localhost:11434/v1
EMBEDDING_MODEL=bge-m3
```

### 2. ç”Ÿäº§çŽ¯å¢ƒ

```bash
# ä½¿ç”¨ vLLM + GPUï¼Œé«˜æ€§èƒ½
docker-compose -f docker-compose.vllm.yml up -d

# é…ç½®
EMBEDDING_API_BASE=http://vllm-embedding:8000/v1
EMBEDDING_MODEL=bge-m3

# å¯ç”¨å¤šå‰¯æœ¬
docker-compose up -d --scale vllm-embedding=2
```

### 3. èµ„æºå—é™çŽ¯å¢ƒ

```bash
# ä½¿ç”¨ Ollama + CPU
docker-compose -f docker-compose.ollama.yml up -d

# æˆ–ä½¿ç”¨ OpenAI API
EMBEDDING_API_BASE=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
```

---

## ðŸ“š ç›¸å…³èµ„æº

- **vLLM æ–‡æ¡£**: https://docs.vllm.ai/
- **Ollama æ–‡æ¡£**: https://ollama.ai/
- **OpenAI API**: https://platform.openai.com/docs/guides/embeddings
- **BGE æ¨¡åž‹**: https://huggingface.co/BAAI/bge-m3

---

## ðŸ“ æ€»ç»“

âœ… **ç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹ API**
âœ… **è§£å†³å¤šè¿›ç¨‹é‡å¤åŠ è½½é—®é¢˜**
âœ… **æ”¯æŒ vLLMã€Ollamaã€OpenAI**
âœ… **å†…å­˜èŠ‚çœ 66%**
âœ… **æž¶æž„æ¸…æ™°ï¼Œæ˜“äºŽç»´æŠ¤**

**æŽ¨èé…ç½®ï¼š**
- **ç”Ÿäº§çŽ¯å¢ƒ** â†’ vLLM + GPU
- **å¼€å‘çŽ¯å¢ƒ** â†’ Ollama + CPU
- **å¿«é€Ÿä¸Šçº¿** â†’ OpenAI API

ç«‹å³å¼€å§‹ä½¿ç”¨ï¼ðŸš€
