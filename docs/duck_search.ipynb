{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# 需要同步jupyter运行异步事件循环\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.enums.embedding import EmbeddingDriverEnum\n",
    "from app.rag.embedding.embeeding_model import EmbeddingFactory\n",
    "\n",
    "EmbeddingFactory.init(\n",
    "    {\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"BAAI/bge-m3\",\n",
    "        \"driver\": EmbeddingDriverEnum.MAC,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import orjson\n",
    "import rich\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "from langchain_community.document_loaders import PlaywrightURLLoader, WebBaseLoader\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchResults\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from app.rag.llm.tokenizers import TokenCounter\n",
    "\n",
    "web_loader_type: Literal[\"playwright\", \"webbase\"] = \"webbase\"\n",
    "\n",
    "\n",
    "def get_web_loader(web_loader_type: Literal[\"playwright\", \"webbase\"], urls: list[str]) -> WebBaseLoader:\n",
    "    match web_loader_type:\n",
    "        case \"playwright\":\n",
    "            return PlaywrightURLLoader(urls, headless=True)\n",
    "        case \"webbase\":\n",
    "            return WebBaseLoader(urls)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid web loader type: {web_loader_type}\")\n",
    "\n",
    "\n",
    "def get_num_tokens(docs: list[Document]) -> int:\n",
    "    return sum(TokenCounter.estimate_tokens(doc.page_content) for doc in docs)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # 替换形如 \"\\n \\n \\n\" 的混合空行为单个 \\n\n",
    "    RE_MIXED_NEWLINES = re.compile(r\"(?:\\s*\\n\\s*){2,}\")\n",
    "    RE_SPACES = re.compile(r\"[ \\t]+\")\n",
    "\n",
    "    text = text.strip()\n",
    "    text = RE_MIXED_NEWLINES.sub(\"\\n\", text)  # 替换混合换行\n",
    "    text = RE_SPACES.sub(\" \", text)  # 合并空格和 tab\n",
    "    return text\n",
    "\n",
    "\n",
    "@tool\n",
    "async def duck_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    search_results = await DuckDuckGoSearchResults(output_format=\"json\", max_results=10).arun(query)\n",
    "    if not search_results:\n",
    "        return \"No search results found\"\n",
    "    search_results = orjson.loads(search_results)\n",
    "    web_loader = get_web_loader(web_loader_type, [x[\"link\"] for x in search_results])\n",
    "    docs: list[Document] = []\n",
    "    async for doc in web_loader.alazy_load():\n",
    "        docs.append(doc)\n",
    "    docs = [Document(page_content=clean_text(doc.page_content), metadata=doc.metadata) for doc in docs]\n",
    "    docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "    embedding = EmbeddingFactory.get()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        docs, embedding=embedding, client_settings=ChromaSettings(anonymized_telemetry=False)\n",
    "    )\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=10)\n",
    "    rich.print(retrieved_docs)\n",
    "    return search_results\n",
    "\n",
    "\n",
    "await duck_search.ainvoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiohttp\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: 定义异步 Tool\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "@tool\n",
    "async def get_ip() -> str:\n",
    "    \"\"\"获取公网 IP\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(\"https://api.ipify.org?format=json\") as resp:\n",
    "            data = await resp.json()\n",
    "            return f\"Your IP is {data['ip']}\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: 节点函数中调用异步 Tool\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "async def tool_node(state: dict) -> dict:\n",
    "    print(\"[tool_node] Running async tool...\")\n",
    "    result = await get_ip.arun(\"What's my IP?\")\n",
    "    print(f\"[tool_node] Tool result: {result}\")\n",
    "    state[\"ip_result\"] = result\n",
    "    return state\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: 构建异步 LangGraph\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    builder = StateGraph(dict)\n",
    "    builder.add_node(\"get_ip\", tool_node)\n",
    "    builder.set_entry_point(\"get_ip\")\n",
    "    builder.add_edge(\"get_ip\", END)\n",
    "    return builder.compile()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: 异步运行图\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "async def main():\n",
    "    graph = build_graph()\n",
    "    final_state = await graph.ainvoke({})\n",
    "    print(\"[main] Final state:\", final_state)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
