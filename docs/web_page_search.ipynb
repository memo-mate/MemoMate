{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è·å–æœ€æ–°å†™æ³•ï¼š** https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/#legacy-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# éœ€è¦åŒæ­¥jupyterè¿è¡Œå¼‚æ­¥äº‹ä»¶å¾ªç¯\n",
    "nest_asyncio.apply()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ Playwright æŠ“å–ç½‘é¡µ\n",
    "uv pip install playwright\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "async def async_load_playwright(url: str) -> str:\n",
    "    \"\"\"ä½¿ç”¨PlayWrightåŠ è½½æŒ‡å®šçš„URLï¼Œå¹¶ä½¿ç”¨BeautifulSoupè§£æã€‚\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    from playwright.async_api import async_playwright\n",
    "\n",
    "    results = \"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            page_source = await page.content()\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "        except Exception as e:\n",
    "            results = f\"é”™è¯¯ï¼š{e}\"\n",
    "        await browser.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_async(coro):\n",
    "    event_loop = asyncio.get_event_loop()\n",
    "    return event_loop.run_until_complete(coro)\n",
    "\n",
    "\n",
    "@tool\n",
    "def browse_web_page(url: str) -> str:\n",
    "    \"\"\"ä»¥è¯¦ç»†çš„æ–¹å¼æŠ“å–æ•´ä¸ªç½‘é¡µã€‚å¯èƒ½ä¼šå¯¼è‡´è§£æé—®é¢˜ã€‚\"\"\"\n",
    "    return run_async(async_load_playwright(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "# ğŸ§© å¯é€‰ï¼šåˆ†å—å‡½æ•°\n",
    "def _get_text_splitter() -> RecursiveCharacterTextSplitter:\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "\n",
    "\n",
    "# ğŸ§  Tool å®ç°\n",
    "class WebpageQATool(BaseTool):\n",
    "    name: str = \"query_webpage\"\n",
    "    description: str = \"æµè§ˆç½‘é¡µå¹¶æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶é™„å¸¦æ¥æºã€‚\"\n",
    "\n",
    "    # å‚æ•°ï¼šæ¨¡å‹ã€åˆ†å—å™¨ã€chainç±»å‹\n",
    "    llm: BaseLanguageModel\n",
    "    chain_type: Literal[\"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"] = \"map_reduce\"\n",
    "    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\n",
    "\n",
    "    def _run(self, url: str, question: str) -> str:\n",
    "        \"\"\"ä½¿ç”¨ QA chain å›ç­”ç½‘é¡µå†…å®¹ä¸­çš„é—®é¢˜ï¼Œå¹¶è¿”å›ç­”æ¡ˆå’Œæ¥æºã€‚\"\"\"\n",
    "\n",
    "        try:\n",
    "            # ğŸ‘‡ 1. æŠ“ç½‘é¡µå†…å®¹ï¼ˆä½ éœ€è¦å®ç°è¿™ä¸ªå‡½æ•°ï¼‰\n",
    "            html = browse_web_page.run(url)\n",
    "\n",
    "            # ğŸ‘‡ 2. å°è£…ä¸º Document + åˆ‡å—\n",
    "            doc = Document(page_content=html, metadata={\"source\": url})\n",
    "            docs = self.text_splitter.split_documents([doc])\n",
    "\n",
    "            # ğŸ‘‡ 3. æ„å»º QA Chainï¼ˆå«æ¥æºå¼•ç”¨ï¼‰\n",
    "            chain = load_qa_with_sources_chain(\n",
    "                llm=self.llm,\n",
    "                chain_type=self.chain_type,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # ğŸ‘‡ 4. è¿è¡Œ QA chain\n",
    "            result = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "\n",
    "            # ğŸ‘‡ 5. è¿”å›æ ¼å¼åŒ–ç»“æœ\n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            sources = result.get(\"sources\", \"\")\n",
    "            return f\"å›ç­”ï¼š{answer}\\n\\næ¥æºï¼š{sources}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}\"\n",
    "\n",
    "    async def _arun(self, url: str, question: str) -> str:\n",
    "        raise NotImplementedError(\"å¼‚æ­¥æš‚æœªå®ç°ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_website_tool = WebpageQATool(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. åŠ è½½ç½‘é¡µï¼ˆæ”¯æŒ JS åŠ¨æ€åŠ è½½çš„é¡µé¢é™¤å¤–ï¼‰\n",
    "url = \"https://en.wikipedia.org/wiki/OpenAI\"\n",
    "loader = WebBaseLoader([url])\n",
    "docs = loader.load()  # è¿”å›çš„æ˜¯ List[Document]\n",
    "\n",
    "# 2. åˆ†å—ï¼ˆå¿…é¡»æ­¥éª¤ï¼Œä¿è¯é€‚é… LLM è¾“å…¥é™åˆ¶ï¼‰\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 3. æ„å»ºå‘é‡ç´¢å¼•\n",
    "vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4. æ„å»º RetrievalQAWithSourcesChain\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # å¯æ”¹ä¸º map_reduce/refine/map_rerank\n",
    "    return_source_documents=True,  # å¯é€‰ï¼šè¿”å›å®Œæ•´æ–‡æ¡£\n",
    ")\n",
    "\n",
    "# 5. æé—®\n",
    "question = \"OpenAI æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ\"\n",
    "result = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "# 6. è¾“å‡ºç»“æœ\n",
    "print(\"ğŸ§  ç­”æ¡ˆï¼š\", result[\"answer\"])\n",
    "print(\"ğŸ“ æ¥æºï¼š\", result[\"sources\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°è£…æˆå·¥å…·\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryWebpageWithSourcesTool(BaseTool):\n",
    "    name: str = \"query_webpage\"\n",
    "    description: str = \"è¯»å–ç½‘é¡µå¹¶å›ç­”é—®é¢˜ï¼Œé™„å¸¦æ¥æº\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    def _run(self, url: str, question: str) -> str:\n",
    "        docs = WebBaseLoader(url).load()\n",
    "        split_docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(docs)\n",
    "        vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm=self.llm, retriever=retriever, return_source_documents=False\n",
    "        )\n",
    "\n",
    "        result = qa_chain.invoke({\"question\": question})\n",
    "        return f\"ğŸ§  å›ç­”ï¼š{result['answer']}\\nğŸ“ æ¥æºï¼š{result['sources']}\"\n",
    "\n",
    "    async def _arun(self, url: str, question: str):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥½çš„ï¼Œä¸‹é¢æ˜¯**å¢å¼ºç‰ˆçš„ç½‘é¡µé—®ç­”å·¥å…· `QueryWebpagesWithSourcesTool`**ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… åŠŸèƒ½äº®ç‚¹ï¼š\n",
    "\n",
    "| åŠŸèƒ½                | æè¿°                                            |\n",
    "| ----------------- | --------------------------------------------- |\n",
    "| ğŸ”— æ”¯æŒå¤šä¸ªç½‘é¡µé“¾æ¥       | å¤šç½‘é¡µåˆå¹¶æé—®ï¼Œä¸€æ¬¡é—®å¤šä¸ªç½‘é¡µå†…å®¹                             |\n",
    "| ğŸ“„ è‡ªåŠ¨ç½‘é¡µè§£æ         | ç”¨ `WebBaseLoader` æŠ“ç½‘é¡µæ­£æ–‡                       |\n",
    "| ğŸ§© æ–‡æœ¬åˆ†å— + åµŒå…¥ + æ£€ç´¢ | è‡ªåŠ¨æ„å»ºå‘é‡æ•°æ®åº“                                     |\n",
    "| ğŸ§  æ”¯æŒå¤šç§ QA æ¨¡å¼     | `stuff`, `map_reduce`, `refine`, `map_rerank` |\n",
    "| ğŸ“ æ¥æºå¼•ç”¨           | è¾“å‡ºä¸­åŒ…å«æå–çš„ç½‘é¡µé“¾æ¥ï¼ˆsourceï¼‰                          |\n",
    "| ğŸ§± å¯æ¥å…¥ Agent      | å®Œæ•´ Tool ç»“æ„ï¼Œæ”¯æŒ Tool Calling                    |\n",
    "| âš™ï¸ å¯è¿”å›ç»“æ„åŒ– JSON    | ç­”æ¡ˆ + sources å¯ç›´æ¥å¤„ç†                            |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… å®Œæ•´ä»£ç ï¼ˆå¤šç½‘é¡µé—®ç­”å·¥å…·ï¼‰\n",
    "\n",
    "```python\n",
    "from typing import List, Union, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.schema import Document\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "\n",
    "\n",
    "# ğŸ§  å‚æ•°ç»“æ„ï¼šç”¨äº Tool æ¥æ”¶ inputs\n",
    "class WebQAInput(BaseModel):\n",
    "    urls: Union[str, List[str]] = Field(..., description=\"è¦æé—®çš„ç½‘é¡µé“¾æ¥ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ª\")\n",
    "    question: str = Field(..., description=\"è¦æé—®çš„é—®é¢˜\")\n",
    "\n",
    "\n",
    "# ğŸ› ï¸ Tool å®ç°\n",
    "class QueryWebpagesWithSourcesTool(BaseTool):\n",
    "    name: str = \"query_webpages_with_sources\"\n",
    "    description: str = \"è¯»å–ä¸€ä¸ªæˆ–å¤šä¸ªç½‘é¡µå¹¶å›ç­”é—®é¢˜ï¼Œé™„å¸¦å¼•ç”¨æ¥æº\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    chain_type: Literal[\"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"] = \"map_reduce\"\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 100\n",
    "\n",
    "    args_schema = WebQAInput\n",
    "\n",
    "    def _run(self, urls: Union[str, List[str]], question: str) -> str:\n",
    "        try:\n",
    "            # æ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨\n",
    "            url_list = [urls] if isinstance(urls, str) else urls\n",
    "\n",
    "            # Step 1: åŠ è½½ç½‘é¡µ\n",
    "            docs: List[Document] = []\n",
    "            for url in url_list:\n",
    "                web_docs = WebBaseLoader(url).load()\n",
    "                for d in web_docs:\n",
    "                    d.metadata[\"source\"] = url  # æ˜ç¡®æ¥æº\n",
    "                docs.extend(web_docs)\n",
    "\n",
    "            # Step 2: åˆ†å—\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap\n",
    "            )\n",
    "            split_docs = splitter.split_documents(docs)\n",
    "\n",
    "            # Step 3: å‘é‡ç´¢å¼•\n",
    "            vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "            retriever = vectorstore.as_retriever()\n",
    "\n",
    "            # Step 4: QA Chain\n",
    "            qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                retriever=retriever,\n",
    "                chain_type=self.chain_type,\n",
    "                return_source_documents=False\n",
    "            )\n",
    "\n",
    "            result = qa_chain.invoke({\"question\": question})\n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            sources = result.get(\"sources\", \"\")\n",
    "\n",
    "            return f\"ğŸ§  å›ç­”ï¼š{answer}\\nğŸ“ æ¥æºï¼š{sources}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ å‡ºé”™ï¼š{str(e)}\"\n",
    "\n",
    "    async def _arun(self, urls: Union[str, List[str]], question: str) -> str:\n",
    "        raise NotImplementedError(\"æš‚ä¸æ”¯æŒå¼‚æ­¥è¿è¡Œ\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ç¤ºä¾‹ä½¿ç”¨æ–¹å¼ï¼š\n",
    "\n",
    "```python\n",
    "tool = QueryWebpagesWithSourcesTool(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"map_rerank\"\n",
    ")\n",
    "\n",
    "output = tool.run({\n",
    "    \"urls\": [\n",
    "        \"https://en.wikipedia.org/wiki/OpenAI\",\n",
    "        \"https://en.wikipedia.org/wiki/ChatGPT\"\n",
    "    ],\n",
    "    \"question\": \"OpenAI å’Œ ChatGPT æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\"\n",
    "})\n",
    "\n",
    "print(output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ç¤ºä¾‹è¾“å‡ºï¼š\n",
    "\n",
    "```\n",
    "ğŸ§  å›ç­”ï¼šOpenAI æ˜¯ ChatGPT çš„å¼€å‘è€…ã€‚ChatGPT æ˜¯ç”± OpenAI å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚\n",
    "ğŸ“ æ¥æºï¼šhttps://en.wikipedia.org/wiki/OpenAI, https://en.wikipedia.org/wiki/ChatGPT\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ‰©å±•å»ºè®®ï¼ˆå¯é€‰ï¼‰ï¼š\n",
    "\n",
    "| æ‰©å±•          | å»ºè®®åšæ³•                                          |\n",
    "| ----------- | --------------------------------------------- |\n",
    "| æ”¯æŒ JSON è¾“å‡º  | æ”¹æˆ `return {\"answer\": ..., \"sources\": ...}`   |\n",
    "| æ”¯æŒ Agent è°ƒç”¨ | è®¾ç½® `tool.description` å’Œ `args_schema`         |\n",
    "| å¼‚æ­¥æŠ“ç½‘é¡µ       | ç”¨ `aiohttp + asyncio.gather` æ›¿ä»£ WebBaseLoader |\n",
    "| å¢åŠ æ—¥å¿—        | `print(f\"Loaded from {url}\")` ç­‰è°ƒè¯•è¾…åŠ©           |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© å°è´´å£«ï¼šå¦‚æœç½‘é¡µ JS æ¸²æŸ“æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "`WebBaseLoader` ä¸èƒ½æŠ“åŠ¨æ€ JS å†…å®¹ï¼Œä½ å¯ä»¥æ›¿æ¢ä¸ºï¼š\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PlaywrightURLLoader\n",
    "\n",
    "loader = PlaywrightURLLoader(urls=[...], headless=True)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ è¡¥å……è¿™æ®µä»£ç çš„ Tool Calling JSON schemaï¼ˆOpenAI functions/assistant API ç”¨ï¼‰ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥½çš„ï¼ä½ æƒ³è¦çš„â€œç½‘é¡µé—®ç­”å·¥å…·â€è¿›ä¸€æ­¥æ‰©å±•æˆ**æ”¯æŒå¤šè½®å¯¹è¯**çš„èƒ½åŠ›ï¼Œå³ï¼š\n",
    "\n",
    "> **è®°ä½ä¹‹å‰çš„é—®é¢˜ä¸ç­”æ¡ˆï¼Œä¸Šä¸‹æ–‡è¿è´¯åœ°å¯¹è¯é—®ç½‘é¡µå†…å®¹**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ ¸å¿ƒæ€è·¯\n",
    "\n",
    "åœ¨åŸæœ‰çš„ç½‘é¡µé—®ç­”åŸºç¡€ä¸Šï¼Œ**åŠ å…¥èŠå¤©å†å²çš„è®°å¿†æœºåˆ¶**ï¼š\n",
    "\n",
    "1. ç»´æŠ¤ä¸€ä»½å¤šè½® `chat_history`ï¼ˆå¯¹è¯ä¸Šä¸‹æ–‡ï¼‰\n",
    "2. åœ¨æ¯æ¬¡æé—®æ—¶ï¼ŒæŠŠè¿‡å»çš„é—®ç­”ä¹Ÿä¸€å¹¶è¾“å…¥æ¨¡å‹\n",
    "3. ä¿è¯ç­”æ¡ˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä½¿ç”¨ç»„ä»¶è¯´æ˜\n",
    "\n",
    "| ç»„ä»¶                             | ç”¨æ³•                        |\n",
    "| ------------------------------ | ------------------------- |\n",
    "| `ConversationalRetrievalChain` | å®˜æ–¹æ¨èé“¾ï¼Œæ”¯æŒå†å²å¯¹è¯è®°å¿†            |\n",
    "| `chat_history`                 | ä¼ å…¥ `(user, ai)` å¯¹åˆ—è¡¨ï¼Œæ„æˆä¸Šä¸‹æ–‡ |\n",
    "| `Retriever`                    | ä½¿ç”¨å‘é‡æ£€ç´¢æ„å»ºï¼ˆä»ç½‘é¡µæ–‡æœ¬æ„å»ºï¼‰         |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… å®Œæ•´å¤šè½®å¯¹è¯ç‰ˆ Web QA Tool\n",
    "\n",
    "```python\n",
    "from typing import List, Tuple, Union, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import Document\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "\n",
    "\n",
    "class ConversationalWebQAInput(BaseModel):\n",
    "    urls: Union[str, List[str]] = Field(..., description=\"ç½‘é¡µé“¾æ¥ï¼ˆå•ä¸ªæˆ–å¤šä¸ªï¼‰\")\n",
    "    question: str = Field(..., description=\"å½“å‰ç”¨æˆ·æé—®\")\n",
    "    history: List[Tuple[str, str]] = Field(default_factory=list, description=\"å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º[(user, ai), ...]\")\n",
    "\n",
    "\n",
    "class ConversationalWebQATool(BaseTool):\n",
    "    name: str = \"conversational_web_qa\"\n",
    "    description: str = \"è¯»å–ç½‘é¡µå¹¶è¿›è¡Œå¤šè½®å¯¹è¯é—®ç­”ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†å’Œæ¥æºå¼•ç”¨\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 100\n",
    "\n",
    "    args_schema = ConversationalWebQAInput\n",
    "\n",
    "    def _run(self, urls: Union[str, List[str]], question: str, history: List[Tuple[str, str]]) -> str:\n",
    "        try:\n",
    "            url_list = [urls] if isinstance(urls, str) else urls\n",
    "\n",
    "            # Step 1: åŠ è½½ç½‘é¡µ\n",
    "            docs: List[Document] = []\n",
    "            for url in url_list:\n",
    "                web_docs = WebBaseLoader(url).load()\n",
    "                for d in web_docs:\n",
    "                    d.metadata[\"source\"] = url\n",
    "                docs.extend(web_docs)\n",
    "\n",
    "            # Step 2: æ–‡æœ¬åˆ‡å—\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            split_docs = splitter.split_documents(docs)\n",
    "\n",
    "            # Step 3: å‘é‡ç´¢å¼•\n",
    "            vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "            retriever = vectorstore.as_retriever()\n",
    "\n",
    "            # Step 4: æ„å»º Conversational QA Chain\n",
    "            qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=self.llm,\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "\n",
    "            # Step 5: å¤šè½®å¯¹è¯\n",
    "            result = qa_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "\n",
    "            # Step 6: è¾“å‡ºç­”æ¡ˆä¸æ¥æº\n",
    "            answer = result[\"answer\"]\n",
    "            sources = \", \".join({doc.metadata.get(\"source\", \"æœªçŸ¥\") for doc in result[\"source_documents\"]})\n",
    "\n",
    "            return f\"ğŸ§  å›ç­”ï¼š{answer}\\nğŸ“ æ¥æºï¼š{sources}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ å‡ºé”™ï¼š{str(e)}\"\n",
    "\n",
    "    async def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"æš‚ä¸æ”¯æŒå¼‚æ­¥\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ¨¡æ‹Ÿå¤šè½®å¯¹è¯ï¼‰\n",
    "\n",
    "```python\n",
    "tool = ConversationalWebQATool(llm=ChatOpenAI(temperature=0))\n",
    "\n",
    "history = []\n",
    "\n",
    "q1 = \"OpenAI æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ\"\n",
    "a1 = tool.run({\"urls\": \"https://en.wikipedia.org/wiki/OpenAI\", \"question\": q1, \"history\": history})\n",
    "print(a1)\n",
    "history.append((q1, a1))\n",
    "\n",
    "q2 = \"ä»–ä»¬çš„è‘—åäº§å“æœ‰å“ªäº›ï¼Ÿ\"\n",
    "a2 = tool.run({\"urls\": \"https://en.wikipedia.org/wiki/OpenAI\", \"question\": q2, \"history\": history})\n",
    "print(a2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ç¤ºä¾‹è¾“å‡ºï¼ˆå¯ä¸Šä¸‹æ–‡è¿è´¯ï¼‰\n",
    "\n",
    "```\n",
    "ğŸ§  å›ç­”ï¼šOpenAI æ˜¯ä¸€å®¶ä»äº‹äººå·¥æ™ºèƒ½ç ”ç©¶çš„å®éªŒå®¤ã€‚\n",
    "ğŸ“ æ¥æºï¼šhttps://en.wikipedia.org/wiki/OpenAI\n",
    "\n",
    "ğŸ§  å›ç­”ï¼šä»–ä»¬çš„è‘—åäº§å“åŒ…æ‹¬ ChatGPTã€Codex å’Œ DALLÂ·Eã€‚\n",
    "ğŸ“ æ¥æºï¼šhttps://en.wikipedia.org/wiki/OpenAI\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ•´ä½“ç»“æ„å›¾\n",
    "\n",
    "```\n",
    "User Question + Chat History\n",
    "        â†“\n",
    " ConversationalRetrievalChain\n",
    "        â†“\n",
    "Retriever (ç½‘é¡µå‘é‡åº“)\n",
    "        â†“\n",
    "å›ç­” + æ¥æº\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æ€»ç»“\n",
    "\n",
    "| åŠŸèƒ½                     | æ˜¯å¦æ”¯æŒ                    |\n",
    "| ---------------------- | ----------------------- |\n",
    "| å¤šç½‘é¡µåˆå¹¶é—®ç­”                | âœ…                       |\n",
    "| ä¸Šä¸‹æ–‡è®°å¿†                  | âœ…ï¼ˆé€šè¿‡ `chat_history`ï¼‰    |\n",
    "| å¤šè½®æé—®è¿è´¯                 | âœ…                       |\n",
    "| å¼•ç”¨æ¥æºç½‘é¡µ                 | âœ…                       |\n",
    "| æ”¯æŒ Agent / ToolCalling | âœ…ï¼ˆç¬¦åˆ LangChain Tool æ¥å£ï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦éœ€è¦æˆ‘å†æ‰©å±•ä¸ºæ”¯æŒ**LangGraph çŠ¶æ€æœºå¼å¤šè½®é—®ç­”**ï¼Œæˆ–è€…æ¥å…¥ LangChain Expression Languageï¼Ÿå¦‚æœä½ æƒ³è®©å®ƒæ¥å…¥çœŸå® Web Agentï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å°è£…ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "from typing import Literal\n",
    "\n",
    "import faiss\n",
    "import rich\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "from app.core import settings\n",
    "\n",
    "\n",
    "# ğŸ§  å‚æ•°ç»“æ„ï¼šç”¨äº Tool æ¥æ”¶ inputs\n",
    "class WebQAInput(BaseModel):\n",
    "    urls: str | list[str] = Field(..., description=\"è¦æé—®çš„ç½‘é¡µé“¾æ¥ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ª\")\n",
    "    question: str = Field(..., description=\"è¦æé—®çš„é—®é¢˜\")\n",
    "\n",
    "\n",
    "# ğŸ› ï¸ Tool å®ç°\n",
    "class QueryWebpagesWithSourcesTool(BaseTool):\n",
    "    name: str = \"query_webpages_with_sources\"\n",
    "    description: str = \"è¯»å–ä¸€ä¸ªæˆ–å¤šä¸ªç½‘é¡µå¹¶å›ç­”é—®é¢˜ï¼Œé™„å¸¦å¼•ç”¨æ¥æº\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    chain_type: Literal[\"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"] = \"map_reduce\"\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 100\n",
    "\n",
    "    args_schema: type[BaseModel] = WebQAInput\n",
    "\n",
    "    def _run(self, urls: str | list[str], question: str) -> str:\n",
    "        try:\n",
    "            # æ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨\n",
    "            url_list = [urls] if isinstance(urls, str) else urls\n",
    "\n",
    "            # Step 1: åŠ è½½ç½‘é¡µ\n",
    "            docs: list[Document] = []\n",
    "            for url in url_list:\n",
    "                web_docs = WebBaseLoader(url).load()\n",
    "                for d in web_docs:\n",
    "                    d.metadata[\"source\"] = url  # æ˜ç¡®æ¥æº\n",
    "                docs.extend(web_docs)\n",
    "\n",
    "            # Step 2: åˆ†å—\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "            split_docs = splitter.split_documents(docs)\n",
    "\n",
    "            # Step 3: å‘é‡ç´¢å¼•\n",
    "            embeddings = OpenAIEmbeddings(\n",
    "                model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "                openai_api_key=settings.SILICONFLOW_API_KEY,\n",
    "                openai_api_base=settings.SILICONFLOW_API_BASE,\n",
    "            )\n",
    "            index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "            vector_store = FAISS(\n",
    "                embedding_function=embeddings,\n",
    "                index=index,\n",
    "                docstore=InMemoryDocstore(),\n",
    "                index_to_docstore_id={},\n",
    "            )\n",
    "            for batch in batched(split_docs, 64):\n",
    "                vector_store.add_documents(batch)\n",
    "            retriever = vector_store.as_retriever()\n",
    "\n",
    "            # Step 4: QA Chain\n",
    "            qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                retriever=retriever,\n",
    "                chain_type=self.chain_type,\n",
    "                return_source_documents=True,\n",
    "            )\n",
    "\n",
    "            result = qa_chain.invoke({\"question\": question})\n",
    "            answer = result.get(\"answer\", \"\")\n",
    "            sources = result.get(\"sources\", \"\")\n",
    "\n",
    "            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° result çš„é”®\n",
    "            print(f\"è°ƒè¯•ï¼šresult åŒ…å«çš„é”®: {list(result.keys())}\")\n",
    "            print(f\"è°ƒè¯•ï¼šåŸå§‹ sources: '{sources}'\")\n",
    "\n",
    "            # å¤šç§æ–¹å¼æå– sources\n",
    "            source_urls = set()\n",
    "\n",
    "            # æ–¹æ³•1ï¼šä» source_documents ä¸­æå–\n",
    "            if \"source_documents\" in result and result[\"source_documents\"]:\n",
    "                for doc in result[\"source_documents\"]:\n",
    "                    if hasattr(doc, \"metadata\") and \"source\" in doc.metadata:\n",
    "                        source_urls.add(doc.metadata[\"source\"])\n",
    "                        print(f\"è°ƒè¯•ï¼šä» source_documents æå–åˆ°: {doc.metadata['source']}\")\n",
    "\n",
    "            # æ–¹æ³•2ï¼šå¦‚æœ sources å­—æ®µä¸ä¸ºç©ºï¼Œè§£æå®ƒ\n",
    "            if sources and sources.strip():\n",
    "                # sources å¯èƒ½æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²\n",
    "                parsed_sources = [s.strip() for s in sources.split(\",\") if s.strip()]\n",
    "                source_urls.update(parsed_sources)\n",
    "                print(f\"è°ƒè¯•ï¼šä» sources å­—æ®µè§£æåˆ°: {parsed_sources}\")\n",
    "\n",
    "            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨åŸå§‹ URL\n",
    "            if not source_urls:\n",
    "                source_urls.update(url_list)\n",
    "                print(f\"è°ƒè¯•ï¼šä½¿ç”¨åŸå§‹ URL: {url_list}\")\n",
    "\n",
    "            final_sources = \", \".join(source_urls) if source_urls else \"æœªæ‰¾åˆ°æ¥æº\"\n",
    "            print(f\"è°ƒè¯•ï¼šæœ€ç»ˆ sources: '{final_sources}'\")\n",
    "\n",
    "            return f\"ğŸ§  å›ç­”ï¼š{answer}\\nğŸ“ æ¥æºï¼š{final_sources}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ å‡ºé”™ï¼š{str(e)}\"\n",
    "\n",
    "    async def _arun(self, urls: str | list[str], question: str) -> str:\n",
    "        raise NotImplementedError(\"æš‚ä¸æ”¯æŒå¼‚æ­¥è¿è¡Œ\")\n",
    "\n",
    "\n",
    "tool = QueryWebpagesWithSourcesTool(\n",
    "    llm=ChatOpenAI(\n",
    "        model=\"Qwen/Qwen3-32B\",\n",
    "        api_key=settings.SILICONFLOW_API_KEY,\n",
    "        base_url=settings.SILICONFLOW_API_BASE,\n",
    "        temperature=0.8,\n",
    "    ),\n",
    "    chain_type=\"map_reduce\",  # æ”¹ä¸º stuff æ¨¡å¼ï¼Œå¯¹ sources è¿”å›æ›´ç¨³å®š\n",
    ")\n",
    "\n",
    "output = tool.run(\n",
    "    {\n",
    "        \"urls\": [\"https://en.wikipedia.org/wiki/OpenAI\", \"https://en.wikipedia.org/wiki/ChatGPT\"],\n",
    "        \"question\": \"OpenAI å’Œ ChatGPT æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# rich.print(output)\n",
    "# richæ¸²æŸ“æˆ markdown æ‰“å°\n",
    "console = Console()\n",
    "console.print(Markdown(output))\n",
    "# \"\"\"ğŸ”§ å¦‚æœä½ æƒ³å®šåˆ¶å®ƒ\n",
    "# ä½ å¯ä»¥æ›¿æ¢å…¶ä¸­ä»»æ„ç»„ä»¶ï¼š\n",
    "# æ›¿æ¢ retrieverï¼ˆå¦‚æ¢æˆ MultiVectorRetriever, SelfQueryRetrieverï¼‰\n",
    "# è‡ªå®šä¹‰ prompt\n",
    "# ä½¿ç”¨ ChatPromptTemplate + StructuredOutputParser æ§åˆ¶ç»“æ„åŒ–è¾“å‡º\n",
    "# æ›¿æ¢ chain_type é€»è¾‘ä¸ºæ›´å¼ºçš„ LangGraph çŠ¶æ€æœº\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¿®å¤åçš„å·¥å…·\n",
    "print(\"=== æµ‹è¯•ä¿®å¤åçš„å·¥å…· ===\")\n",
    "output = tool.run(\n",
    "    {\n",
    "        \"urls\": [\"https://en.wikipedia.org/wiki/OpenAI\", \"https://en.wikipedia.org/wiki/ChatGPT\"],\n",
    "        \"question\": \"OpenAI å’Œ ChatGPT æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\",\n",
    "    }\n",
    ")\n",
    "\n",
    "rich.print(output)\n",
    "\n",
    "# å¦‚æœè¿˜æ˜¯æ²¡æœ‰ sourcesï¼Œå°è¯•å•ä¸ª URL æµ‹è¯•\n",
    "print(\"\\n=== å•ä¸ª URL æµ‹è¯• ===\")\n",
    "output_single = tool.run({\"urls\": \"https://en.wikipedia.org/wiki/OpenAI\", \"question\": \"OpenAI æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ\"})\n",
    "\n",
    "rich.print(output_single)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
